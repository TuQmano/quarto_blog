[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Se va la segunda‚Ä¶",
    "section": "",
    "text": "Con √©ste, el primer posteo en Quarto, este TuQmano arranca su en√©sima √©poca blogueando. El momento no es casual‚Ä¶ hoy es formalmente mi √∫ltimo d√≠a como Director Nacional de Mercados y Estad√≠sitca del Ministerio de Turismo y Deportes de Argentina (o como me gusta m√°s referenciar, el responsable del Sistema de Informaci√≥n Tur√≠stica de la Argentina #SINTA)\nPromet√≠ hace unos d√≠as que iba a contar en extenso sobre estos casi tres a√±os desarrollando el #SINTA , motivo principal del por qu√© no estuve blogueando mucho en tuQmanoR. Cuando se trataba de estar generando contenido, en diversos soportes, lo estuvimos haciendo concentrando los esfuerzos en yverta.tur.ar/sinta.\n\nManual de Procedimientos DNMyE\nUna decisi√≥n temprana que tomamos cunado empezamos a trabajar con el equipo, que visto hoy celebro y creo fundamental, fue la de tomarnos tiempo para documentar los procesos. El contenedor final de ello es el documento que ac√° comparto, que fue pensado como una herramienta viva, actualizable, que sirva como hoja de ruta para el equipo; y tambi√©n como una referencia hacia afuera de c√≥mo trabajamos.\nDice el Manual de Procedimientos :\n\nLa DNMyE se caracteriza por (i) ser una organizaci√≥n productora de una gran cantidad de datos; (ii) con un equipo altamente calificado t√©cnicamente (en campos de la estad√≠stica, desde ya, y tambi√©n con amplio conocimiento de dominio tur√≠stico). Esto suced√≠a al mismo tiempo que otras caracter√≠sticas que disminu√≠an el impacto potencial de esos recursos valiosos. A saber: (iii) una baja explotaci√≥n y exposici√≥n del gran conjunto de datos, estad√≠sticas e investigaciones desarrolladas; (iv) una gran especializaci√≥n tem√°tica y de responsabilidades de analistas - con documentaci√≥n p√∫blica y publicada limitada - generando una alta dependencia de personas individuales.\nDe manera simplificada y a modo de ilustraci√≥n, podemos pensar en un esquema como el siguiente, con procesos aislados, individuales y m√°s bien cerrados:\n\nDe un tiempo a esta parte, el flujo de trabajo de la DNMyE se ha ido transformando con la idea de generar procesos m√°s abiertos, colaborativos y reproducibles. El diagrama que sigue intenta ilustrar esta transformaci√≥n. Los flujos son ahora m√°s interdependientes, se desarrollaron infraestructuras para el almacenamiento y procesamiento de datos y nuevas plataformas para la comunicaci√≥n de la informaci√≥n, entre otras.\n\n\n\nInternamente, la DNMyE se organiza a partir de 3 grandes equipos o √°reas que, aunque en los hechos no sean compartimentos estancos y el dise√±o se propone transversal, pueden ser definidas por su misi√≥n primaria:\n\nECONOM√çA: realizar seguimiento de indicadores de actividad del sector en particular y la econom√≠a en general; generar reportes de evaluaci√≥n de pol√≠ticas y proyecciones. Ejemplo de estos procesos se encuentran en el Cap√≠tulo 3;\nESTAD√çSTICA: hacer seguimiento y coordinaci√≥n de los operativos estad√≠sticos (ETI, EOH, EVyTH), la producci√≥n de la Cuenta Sat√©lite de Turismo de Argentina (CST-A) y del trabajo con equipos de datos o estad√≠sticas de turismo en las jurisdicciones (provincias y municipios), tal como se detalla en el Cap√≠tulo 1;\nDATOS: proponer flujos de trabajo reproducibles y sistematizados; generaci√≥n de software, tableros y reportes; desarrollo de nuevas fuentes de datos.\n\n\n\n\nEl diagrama anterior resume en buena medida estas mesas de trabajo de la DNMyE y agrega dos espacios m√°s, vinculados en buena medida con el trabajo del equipo de DATOS:\n\nARMONIZACI√ìN - organiza el trabajo federal que se hace con provincias y municipios y llevando adelante el desarrollo del Sistema Federal de Gesti√≥n de Datos Tur√≠sticos de la Argentina - &lt;data&gt;TUR;\nCOMUNICACI√ìN - aglutina flujos de trabajo y procesos detr√°s del Sistema de Informaci√≥n Tur√≠stica de la Argentina (SINTA), la plataforma que desarrollamos y mantenemos para atacar el d√©ficit de comunicaci√≥n y democratizar el acceso a la producci√≥n de la DNMyE (ver Cap√≠tulo 4).\n\n\n(Una versi√≥n de este desarrollo se present√≥ como ponencia en la conferencia internacional csv,conf,v7 que tuvo lugar en la Ciudad de Buenos Aires los d√≠as 19 y 20 de abril de 2023) 1\n\n\nEntonces, dejo ac√° este documento vivo, que resume en buena medida buena parte de lo que trabajamos estos casi tres a√±os con el maravilloso equipo que me toco en suerte conducir, y con el cual contamos con el apoyo y acompa√±amiento de todo el equipo de gesti√≥n que arm√≥ Mat√≠as Lammens al frente del MINTURDEP. En su nombre, muchas gracias a todoxs, de quienes aprend√≠ mucho y me llevo el mejor de los recuerdos.\nFue un gran placer. Y, para m√≠, un gran orgullo el trabajo realizado.\n\nSeguimos!"
  },
  {
    "objectID": "posts/Bienvenidos_a_TuQmanoR.html",
    "href": "posts/Bienvenidos_a_TuQmanoR.html",
    "title": "Bienvenidos a TuQmanoR",
    "section": "",
    "text": "Este nuevo espacio TuQmano en la nube est√° pensado para compartir parte del aprendizaje y trabajo con datos. La idea principal es publicar an√°lisis y visualizaciones desarrolados en R.\nComo primer ejemplo vale este mismo sitio. TuQmanoR est√° creado a partir de la librer√≠a blogdown, que combina diversos lenguages (md, RMd, git y HUGO entre otros) para crear sitios est√°ticos. Todo desde el mismo entorno de RStudio.\nUna de las mayores ventajas de trabajar con R (adem√°s de la gratuidad) es la gran comunidad de usuarios. Para la creaci√≥n de este espacio fueron de gran utilidad:\n\nPara blogdown el libro de Yihui Xie, Amber Thomas y Alison Presmanes Hill: blogdown: Creating Websites with R Markdown.\nPara trabajar con git Happy Git and GitHub for the useR, de Jenny Bryan.\nPara la configuraci√≥n del sitio -vinculando HUGO con github: Making a Website Using Blogdown, Hugo, and GitHub pages, de Amber Thomas.\nla ayuda de useRs de la comunidad argenta ( Elio Campitelli, Fernando Flores y Pao Corrales).\n\nLa comunidad de usuarios de R en Argentina viene creciendo sostenidamente. Experiencias como las de LatinR 2018 y la proliferaci√≥n de grupos como RLadies o R en Buenos Aires (y otros tantos cap√≠tulos en otras ciudades) son una peque√±a muestra de eso.\nPrecisamente en LatinR 2018 pude presentar una parte de mi trabajo - a√∫n un wip. Se trata de geofacet_ARG: una simulaci√≥n de grillas de ggplot2 (facet) como si fueran mapas. El abstract, c√≥digo y presentaci√≥n se encuentran en una carpeta del repisotrio de github. Abajo un ejemplo de un gr√°fico de Tucum√°n (que pienso desarrollar en un pr√≥ximo post) que muestra el % de votos de los 2 principales espacios pol√≠ticos, por departamento, a lo largo del tiempo."
  },
  {
    "objectID": "posts/2020-12-30-un-2020-con-mucho-amor.html",
    "href": "posts/2020-12-30-un-2020-con-mucho-amor.html",
    "title": "Un 2020 con mucho amoR",
    "section": "",
    "text": "Vamos terminando un nuevo a√±o. Uno muy extra√±o y lleno de dificultades para todo el mundo. En esta breve entrada quiero cerrar mi balance en relaci√≥n a una de las actividades/tareas/hobbies que m√°s tiempo dedico d√≠a a d√≠a: estudiar, explorar, ense√±ar y escribir c√≥digo con R. Voy entonces a destacar algunas de estas cosas.\n\n\nDiscuRsos\nCon mis compa√±eros de mentaComunicaci√≥n empezamos a explorar cuantitativamente los discursos de apertura de sesiones de los presidentes argentinos. En este post analizamos los discursos de todos los presidentes desde el retorno de la democracia.\n\n\n\n\n\n\n\n\n\nEste primer ejercicio fue el punto de partida de un trabajo que se sumo a {polAr} y que cont√≥ con una presentaci√≥n en #LatinR2020\n\n\n\nShiny Contest\nParticipamos tambi√©n del 2020 Shiny Contest con una app que permite explorar las caracter√≠sticas socioeconom√≠cas de Argentina a nivel de los radios censales de Argentina.\n\n\n\n\n\n\n\n\n\n\n\n\nüì¶ {geofaceteAR}\nEste a√±o avanzamos finalmente en el desarrollo de paquetes. El primer paso fue la organizaci√≥n del proyecto de grillas como mapas presentado en la primera edici√≥n de LatinR en 2018 en un paquete propio. As√≠ naci√≥ {geofacetAR}, luego incorporado tambi√©n en {polAr}.\n\n\n\n\n\n\n\n\n\nhttps://electorarg.github.io/geofaceteAR/\n\n\n\nüì¶ {polAr}\nEl desarrollo de {polAr} fue un peque√±o paso para el an√°lisis pol√≠tico de Argentina y un gran paso para la TuQmananidad, dado que pudimos cumplir con el objetivo de ser aceptados en CRAN.\nEl paquete naci√≥ como evoluci√≥n de trabajos previos como Inteligencia Electoral, una Shiny App que desarrollamos con mi amigo Juan Pablo Pilorget que pudimos presentar en la segunda edici√≥n de LatinR (Santiago de Chile, 2019).\n\n\n\n\n\n\n\n\n\nLa primera versi√≥n en desarrollo del paquete conten√≠a b√°sicamente funciones para poder facilitar el acceso a datos electorales. Sobre ello se agregaron luego funciones para calcular indicadores y visualizar resultados.\nSe sumaron adem√°s otros flujos de trabajo: (i) funciones para trabajar con geo (con la incoporaci√≥n de funciones de {geopfaceteAR} y de poligonos de distritos); (ii) los mencionados discursos presidenciales; y (iii) un primer avance con datos de votaciones legislativas.\nhttps://electorarg.github.io/polAr/\nAprovechando las funciones que permiten acceder a datos y visualizar resultados electorales, con Camila Higa (mentaComunicaci√≥n) desarrollamos un bot de Twitter que permite consultar los resultados de las elecciones disponibles.\n\n\n\n#MetdosCiPol\nPor tercer a√±o consecutivo dictamos la materia Herramientas Cuantitativas para el An√°lisis Pol√≠tico de la Maestr√≠a en Ciencia Pol√≠tica de la Universidad Torcuato Di Tella. Tenemos la fortuna de que es una materia del tercer trimestre (Sep - Nov), por lo que pudimos aprender del esfuerzo que muches hicieron para virtualizar sus propias materias e incorporar sus ense√±anzas. Un proyecto central en este sentido es MetaDocencia.\nEste contexto nos llev√≥ a crear un sitio web (desarrollado integramente con R) propio para poder organizar la cursada y compartir el desarrollo:\n\n /MetodosCiPol/\n\n\n /MetodosCiPol/\n\n\n\n\nLatinR2020\nTercera edici√≥n de la Conferencia Latinoamericana sobre Uso de R en Investigaci√≥n + Desarrollo, popularmente conocida como LatinR que se esperaba fuera en Montevideo en el mes de septiembre de 2020, pero Covid19.\nEl contexto llev√≥ a que les organizadores decidieran armar una (GRAN) conferencia virtual. Como puse en una entrada al finalizar la conferencia:\n\nLatinR para m√≠ representa lo mejor de la comunidad de usuaRios: apertura, predisposici√≥n para ayudar y colaborar y diversidad son pilares sobre los que se apoya la conferencia. Ejemplo de ello result√≥ la reuni√≥n de muches para trabajar en conjunto propuestas para la rstudioconf::global(2021). Y creo tambi√©n que LatinR demostr√≥, por el gran trabajo de chairs, comit√© cient√≠fico y organizador, que se puede hacer una conferencia potenciando la diversidad y la apertura a nuevos usuarios, que en general tienen mayores restricciones de todo tipo para ser parte.\n\nTuvimos la suerte de poder participar en tres paneles distintos, mostrando mucho de lo hasta ac√° rese√±ado:\n{polAr}: An√°lisis de Datos Pol√≠icos de Argentina\nAbstract | üìä Slides | üì¶ {polAr} | ‚å®Ô∏è blogpost | üìº Video.\nMinaR los discuRsos pResidenciales (con Camila Higa y Lucas Enrich)\nAbstract | üìä Slides | ‚å®Ô∏è blogpost | üìº Video.\n#Tuit√≥metroNacional: monitor de la conversaci√≥n pol√≠tica en Argentina (con Camila Higa).\nAbstract | üìä Slides | ‚å®Ô∏è blogpost | üìº Video | üíª App.\n\nEsperamos que el a√±o que est√° por comenzar nos deje seguir avanzando y aprendiendo al menos una parte de todo lo que pudimos hacer en este dificil 2020. Ah√≠ nos vemos.\nSal√∫ !"
  },
  {
    "objectID": "posts/2020-05-22-empaquetar.html",
    "href": "posts/2020-05-22-empaquetar.html",
    "title": "polaR on CRAN",
    "section": "",
    "text": "Publicaron en CRAN una primera versi√≥n del paquete polAr: -POL√≠tica ARgentina usando R. En este post intentar√© detallar un poco m√°s el proceso de trabajo con el que llegamos hasta ac√° y, como yapa, el de geofaceteAr."
  },
  {
    "objectID": "posts/2020-05-22-empaquetar.html#polar",
    "href": "posts/2020-05-22-empaquetar.html#polar",
    "title": "polaR on CRAN",
    "section": "polAr",
    "text": "polAr\n\nLa idea del paquete es brindar herramientas que faciliten el flujo de trabajo del an√°lisis pol√≠tico - electoral y el acceso a datos de Argentina usando R. Podriamos dividir las funciones de esta primera versi√≥n en tres familias: (i) datos, (ii) indicadores y (iii) visualizaci√≥n:\n\ndatos\nEl paquete no incluye datos, sino que funciona como motor de b√∫squeda y descarga desde un repositorio alternativo. La idea detr√°s de ello es que funcione de modo indpendiente, que √©sta pudiera ser aumentada o modificada sin efectos directos sobre el paquete. En una pr√≥xima publicaci√≥n intetnar√© avanzar sobre la documentaci√≥n de este repositorio y un proceo de chequeo de informaci√≥n.\nLa principal fuente de informaci√≥n es el siempre √∫til Atlas Electoral de Andy Tow. Sitio que, adem√°s de ser la fuente de tabulados electorales m√°s completa que existe en Argentina, disponibiliza las bases con los datos desagregados publicados por los escrutinios provisorios desde el a√±o 2003 en adelante.\nEl primer paso de lo ac√° compartido fue el procesamiento de esas bases de datos y el armado de un repositorio propio que me facilitara el trabajo de llamado de elecciones. En resumen: transformamos archivos .mdb (formato para tablas relacionles de Microsoft Acces) en sqlite para cada a√±o electoral e hicimos consultas a esas basese de datos para obtener un archivo .csv para cada elecci√≥n con una estrcutura as√≠: distrito_categoria_turno_a√±o.csv.\nSolo para las elecciones del proceso electoral de 2019 trabajamos con datos compartidos por pmoracho en github. Algo m√°s de detalle del estado actual de todo esto esta disponible en el repositorio PolAr_Data.\nAl d√≠a de hoy el repositorio cuenta con 425 archvios de elecciones para las catgor√≠as a Presidente, Diputades y Senadores Nacionales, para elecciones generales, primarias (P.A.S.O.) y balotaje presidencial.\nCon show_available_elections() accedemos a un √≠ndice con la informaci√≥n disponible que podemos descargar:\n\n\n# A tibble: 426 √ó 5\n   district category round  year  NOMBRE   \n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    \n 1 arg      presi    balota 2015  ARGENTINA\n 2 arg      presi    gral   2003  ARGENTINA\n 3 arg      presi    gral   2007  ARGENTINA\n 4 arg      presi    gral   2011  ARGENTINA\n 5 arg      presi    gral   2015  ARGENTINA\n 6 arg      presi    gral   2019  ARGENTINA\n 7 arg      presi    paso   2011  ARGENTINA\n 8 arg      presi    paso   2015  ARGENTINA\n 9 arg      presi    paso   2019  ARGENTINA\n10 caba     dip      gral   2005  CABA     \n# ‚Ñπ 416 more rows\n\n\nTomando los par√°metros de la tabla anterior con el siguiente comando podemos completar los campos obligatorios para obtener los datos, en este caso de la elecci√≥n general para presidente de 2019.\n\narg19 &lt;- get_election_data(district = \"arg\",\n                  category = \"presi\",\n                  round = \"gral\", \n                  year = 2019)\n\narg19\n\n# A tibble: 192 √ó 8\n# Groups:   codprov [24]\n   category round  year codprov name_prov    electores listas    votos\n   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 presi    gral   2019 01      CABA           2562670 00010     13662\n 2 presi    gral   2019 01      CABA           2562670 00002     58652\n 3 presi    gral   2019 01      CABA           2562670 00009   1051116\n 4 presi    gral   2019 01      CABA           2562670 00005    714820\n 5 presi    gral   2019 01      CABA           2562670 00001    129933\n 6 presi    gral   2019 01      CABA           2562670 00004     37765\n 7 presi    gral   2019 01      CABA           2562670 blancos   30994\n 8 presi    gral   2019 01      CABA           2562670 nulos     14986\n 9 presi    gral   2019 02      BUENOS AIRES  12504537 00010    149613\n10 presi    gral   2019 02      BUENOS AIRES  12504537 00002    272846\n# ‚Ñπ 182 more rows\n\n\narg19 es un tibble de \\(192\\) filas y 8 variables. Las filas son el producto de 8 filas √∫nicas por provincia (los 24 grupos) con la cantidad de votos obtenidos por las distintas opciones electorales en este turno, categor√≠a y a√±o.\nEn este art√≠culo se puede encontrar m√°s detalles de la funci√≥n.\n\n\n\nindicadores\nEl acceso a la informaci√≥n es quiz√°s la funci√≥n princiapl de polAr, la cual permitir√° a usuaries trabajar con los datos como quieran. Pero el paquete incopora adem√°s funciones para realizar c√°lculos de inter√©s para el an√°lisis pol√≠tico. Esta primera versi√≥n solamente incluye dos varianes: compute_nep(), que devulelve el N√∫mero Efectivo de Partidos (seg√∫n dos f√≥rmulas distintas) y compute_cometitiveness(), que calcula el nivel de competencia de una elecci√≥n determinada.\n\narg19 %&gt;% \n  compute_competitiveness() %&gt;% \n  print(n= 24)\n\n# A tibble: 24 √ó 5\n   codprov competitividad  year category round\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n 1 01               0.836  2019 presi    gral \n 2 02               0.839  2019 presi    gral \n 3 03               0.790  2019 presi    gral \n 4 04               0.689  2019 presi    gral \n 5 05               0.909  2019 presi    gral \n 6 06               0.801  2019 presi    gral \n 7 07               0.778  2019 presi    gral \n 8 08               0.999  2019 presi    gral \n 9 09               0.637  2019 presi    gral \n10 10               0.954  2019 presi    gral \n11 11               0.878  2019 presi    gral \n12 12               0.977  2019 presi    gral \n13 13               0.881  2019 presi    gral \n14 14               0.767  2019 presi    gral \n15 15               0.900  2019 presi    gral \n16 16               0.725  2019 presi    gral \n17 17               0.862  2019 presi    gral \n18 18               0.825  2019 presi    gral \n19 19               0.967  2019 presi    gral \n20 20               0.696  2019 presi    gral \n21 21               0.993  2019 presi    gral \n22 22               0.442  2019 presi    gral \n23 23               0.765  2019 presi    gral \n24 24               0.701  2019 presi    gral \n\n\nEste art√≠culo del paquete tiene algo m√°s de detalle.\n\n\n\nvisualizaci√≥n\nPor √∫ltimo polAr incluye un par funciones que ayudan a visualizar r√°pidamente las elecciones de manera resumida. Una primera alternativa es generar tabulados con los resultados agregados de un comicio:\narg19 %&gt;% \n  get_names() %&gt;% \n  tabulate_results()\n\n\n\n\n\n\n\n\nArgentina - 2019\n\n\nElecci√≥n General - Presidente de la Naci√≥n\n\n\nLista\nVotos\n\n\n\n\nblancos\n64.4%\n\n\nnulos\n35.6%\n\n\n\nFuente: polArverse - https://politicaargentina.github.io/electorAr\n\n\n\n\n\n\n\n\nOtra alternativa es analizarlos de manera gr√°fica1:\narg19 %&gt;% \n  get_names() %&gt;% \n  plot_results(national = T)\n\n\n\n\n\n\n\n\n\nAmbas funciones requieren utilizar primero get_names en la versi√≥n de CRAN. Esta es una funci√≥n auxiliar para vincular la base de datos de resultados con los nombres de las listas que compiten.\nLa versi√≥n en desarrollo elimina ese paso intermedio para los casos por defecto de obtenci√≥n de datos (con fomato long) pero sigue funcionando para los casos en que los datos de la elecci√≥n fueron descargados wide.\nPuede notarse que plot_results() fue llamado con un par√°matro national = TRUE. De esta manera, para elecciones a presidente, el gr√°fico resultante muestra el agregado de votos al nivel del distrito que se elige el cargo. Si, en cambio, no especificamos nada, el par√°metro FALSE por defecto nos devolver√° los resultados a nivel de provncias en grillas c√≥mo si fueran mapas. De igual modo, si descargamos una elecci√≥n a nivel departamental, plot_results()autom√°ticamente graficar√° los resultados con geofacetAR.\n\nget_election_data(district = \"caba\",\n                  category = \"dip\", \n                  round = \"paso\" , \n                  year = 2019 , \n                  level = \"departamento\") %&gt;% \n  plot_results()\n\n\n\n\n\n\n\n\nRESUMIENDO"
  },
  {
    "objectID": "posts/2020-05-22-empaquetar.html#gofacetear",
    "href": "posts/2020-05-22-empaquetar.html#gofacetear",
    "title": "polaR on CRAN",
    "section": "gofaceteAr",
    "text": "gofaceteAr\n\nLo anterior es posible gracias a otro paquete, disponible para descarga desde un repositorio de github. geofaceteAr es una extensi√≥n local del paquete geofacet de Ryan Haffen. El empaquetado fue el resultado de ordenar la aplicaci√≥n para Argentina de esta t√©cnica de acomodar las grillas de ggplot2 como si fueran mapas, originalmente presentadas en LatinR 2018.\n√âste carga el paquete geofacet y agrega funciones de utilidad para la geograf√≠a argentina. La priemra de ellas es show_arg_codes()que muestra un diccionario de grillas con el id correspondiente para descarga y equivalencias de los id correspodientes a los diferentes distritos.\n\n\n# A tibble: 26 √ó 5\n   id           codprov codprov_censo codprov_iso name_iso                      \n   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;                         \n 1 ARGENTINA    \" \"     \" \"           AR          Argentina                     \n 2 CABA         \"01\"    \"02\"          AR-C        Ciudad Aut√≥noma de Buenos Air‚Ä¶\n 3 BUENOS AIRES \"02\"    \"06\"          AR-B        Buenos Aires                  \n 4 CATAMARCA    \"03\"    \"10\"          AR-K        Catamarca                     \n 5 CORDOBA      \"04\"    \"14\"          AR-X        C√≥rdoba                       \n 6 CORRIENTES   \"05\"    \"18\"          AR-W        Corrientes                    \n 7 CHACO        \"06\"    \"22\"          AR-H        Chaco                         \n 8 CHUBUT       \"07\"    \"26\"          AR-U        Chubut                        \n 9 ENTRE RIOS   \"08\"    \"30\"          AR-E        Entre R√≠os                    \n10 FORMOSA      \"09\"    \"34\"          AR-P        Formosa                       \n# ‚Ñπ 16 more rows\n\n\n\nlibrary(geofaceteAR)\n\nshow_arg_codes()\n\nUsamos la funci√≥n get_grid() para llamar una grilla disponible en la tabla anterior. Y grid_preview() si queremos ver el dise√±o de la grilla.\n\nget_grid(\"TUCUMAN\") \n\n   name_provincia            name row col code\n1         TUCUMAN       BURRUYACU   1   4  013\n2         TUCUMAN         CAPITAL   2   4  001\n3         TUCUMAN     CHICLIGASTA   4   2  005\n4         TUCUMAN       CRUZ ALTA   2   5  012\n5         TUCUMAN        FAMAILLA   3   3  003\n6         TUCUMAN        GRANEROS   5   4  009\n7         TUCUMAN JUAN B. ALBERDI   5   3  007\n8         TUCUMAN        LA COCHA   6   3  008\n9         TUCUMAN          LEALES   3   5  011\n10        TUCUMAN           LULES   3   4  002\n11        TUCUMAN        MONTEROS   3   2  004\n12        TUCUMAN       RIO CHICO   4   3  006\n13        TUCUMAN          SIMOCA   4   4  010\n14        TUCUMAN  TAFI DEL VALLE   2   1  017\n15        TUCUMAN      TAFI VIEJO   2   2  016\n16        TUCUMAN         TRANCAS   1   3  014\n17        TUCUMAN     YERBA BUENA   2   3  015\n\n\n\n\n\n\n\n\n\n\n\nLuego, podemos hacer uso nuevamente de la informaci√≥n en nuestro diccionario de identificadores geogr√°ficos para recodificar los distritos. El c√≥digo por default - el que figura dentro de cada cajita del mapa- es el correspondiente a los escrutinios provisorios (que utilizamos en polAr), pero podemos disponer de otros. Por ejemplo, los del INDEC:\n\nget_grid(\"TUCUMAN\") %&gt;%\n  recode_grid(type = \"indec\")\n\n# A tibble: 17 √ó 4\n   name              row   col code \n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 BURRUYACU           1     4 007  \n 2 CAPITAL             2     4 084  \n 3 CHICLIGASTA         4     2 021  \n 4 CRUZ ALTA           2     5 014  \n 5 FAMAILLA            3     3 028  \n 6 GRANEROS            5     4 035  \n 7 JUAN B. ALBERDI     5     3 042  \n 8 LA COCHA            6     3 049  \n 9 LEALES              3     5 056  \n10 LULES               3     4 063  \n11 MONTEROS            3     2 070  \n12 RIO CHICO           4     3 077  \n13 SIMOCA              4     4 091  \n14 TAFI DEL VALLE      2     1 098  \n15 TAFI VIEJO          2     2 105  \n16 TRANCAS             1     3 112  \n17 YERBA BUENA         2     3 119  \n\n\n\nHasta ac√° llegamos con la primera entrada respecto de {polAr} ya formalmente en CRAN. Espero en una pr√≥xima entrada contar un poco m√°s del repositorio de datos y la incorporaci√≥n de nuevas funciones en la versi√≥n de desarrollo."
  },
  {
    "objectID": "posts/2020-05-22-empaquetar.html#footnotes",
    "href": "posts/2020-05-22-empaquetar.html#footnotes",
    "title": "polaR on CRAN",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInspirados en ggplotme de Juan Cruz Rodr√≠gurez con Camila Higa - mentaComunicaci√≥n- trabajamos en un bot de Twitter que permite consultar resultados de elecciones. La idea es que un usuario le pregunta a @pol_ar_bot sobre una elecci√≥n, cumpliendo con los par√°metros necesarios (distrito + turno + a√±o + categoria) y el bot responde graficando el resultado. bot: https://twitter.com/pol_Ar_bot‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2020-03-15-dibujar-legislaturas.html",
    "href": "posts/2020-03-15-dibujar-legislaturas.html",
    "title": "dibujaR legislatuRas",
    "section": "",
    "text": "Aprovechamos el distanciamiento social para comaprtir algunas de las cosas que ven√≠an quedando pendiente de actualizar.\nYa se pueden leer en TuQmanoR dos de los ejercicios con datos y visualizaciones que hicimos con les compa√±eres de menta Comunicaci√≥n:\nEl que sigue es un ejemplo de la actual composici√≥n de la Legislatura de la Provincia de TuQland."
  },
  {
    "objectID": "posts/2020-03-15-dibujar-legislaturas.html#receta-para-una-legislatura-en-3-pasos",
    "href": "posts/2020-03-15-dibujar-legislaturas.html#receta-para-una-legislatura-en-3-pasos",
    "title": "dibujaR legislatuRas",
    "section": "Receta para una legislatuRa en 3 pasos",
    "text": "Receta para una legislatuRa en 3 pasos\n\nEn primer lugar tenemos que armar una base de datos como la de abajo para tener a mano todos los par√°metros que nos permitir√°n customizar lo m√°s posible nuestra vizualizaci√≥n.\n\nEn este caso mantuve los nombres de las columnas en el ejemplo original de la librer√≠a, por lo que muchos valores no tienen demasiado sentido. Estamos hablando de una sola unidad (Tucum√°n) y un solo peri√≥do de tiempo (2020). Pero nuestra base de datos podr√≠a contener, por ejemplo, a los 24 distritos y m√∫ltiples composiciones a lo largo del tiempo. Luego hay variables que funcionan como id (party_short), otra para imprimir las etiquetas (party_long), la cantidad de bancas (seats) y hasta la elecci√≥n de un color que se asocie a la fuerza pol√≠tica en cuesti√≥n.\n\nlibrary(tidyverse)\n\ndiputados_tucuman &lt;- read_csv(\"https://raw.githubusercontent.com/TuQmano/data_TuQmanoR/master/legislatuRas/tucuman.csv\") %&gt;% \n  print()\n\n# A tibble: 7 √ó 9\n   year country house     party_long   party_short seats government colour orden\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  2020 TUCUMAN DIPUTADOS Frente Just‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n2  2020 TUCUMAN DIPUTADOS Fuerza Repu‚Ä¶ FuerzaR         8          0 #0000‚Ä¶     2\n3  2020 TUCUMAN DIPUTADOS Hacemos Tuc‚Ä¶ Hacemos T       2          0 #a6bd‚Ä¶     3\n4  2020 TUCUMAN DIPUTADOS Libres del ‚Ä¶ Libres          1          0 #519e‚Ä¶     1\n5  2020 TUCUMAN DIPUTADOS Psj Recuper‚Ä¶ PSJ             3          0 #f57f‚Ä¶     1\n6  2020 TUCUMAN DIPUTADOS UCRHip√≥lito‚Ä¶ UCRHY           1          0 #f759‚Ä¶     1\n7  2020 TUCUMAN DIPUTADOS UCR (1 banc‚Ä¶ UCR             1          0 #ed00‚Ä¶     1\n\n\n\nEl segundo paso consiste en transformar esa base de datos en otra que tenga un formato adecuado para luego poder realizar la visualizaci√≥n con un agregado de ggplot2. La funci√≥n parliament_data de ggparliament es la que se encarga de la magia. Abajo un detalle de los par√°metros que podemos utilizar.\n\n\n# VOY A CREAR EL OBJETO DE ggparliament con la data ordenada para el plot\n\ndata_diputados_tucuman&lt;- ggparliament::parliament_data(diputados_tucuman, #datos originales\n                                                       type = \"semicircle\", # forma del hemiciclo\n                                                       parl_rows =3, # cantidad de filas\n                                                       party_seats = diputados_tucuman$seats, # n√∫mero de bancas \n                                                       plot_order = diputados_tucuman$orden) %&gt;% #orden de partidos en hemiciclo\n  mutate(colour = as.character(colour)) %&gt;% # vector de texto para codigo HEX de colores asignados previamente\n  as_tibble() %&gt;%  \n  print()\n\n# A tibble: 49 √ó 13\n    year country house     party_long  party_short seats government colour orden\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 2  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 3  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 4  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 5  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 6  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 7  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 8  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n 9  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n10  2020 TUCUMAN DIPUTADOS Frente Jus‚Ä¶ PJ             33          1 #2b8c‚Ä¶     3\n# ‚Ñπ 39 more rows\n# ‚Ñπ 4 more variables: x &lt;dbl&gt;, y &lt;dbl&gt;, row &lt;int&gt;, theta &lt;dbl&gt;\n\n\nObservamos que mientras la base de datos original ten√≠a un largo de \\(7x9\\) (\\(7\\) filas - una por cada partido pol√≠tico- y \\(9\\) columnas o variables), con la transformaci√≥n se cre√≥ un nuevo objeto de \\(49x13\\). Ahora el largo de la base tiene una fila por legislador, extendiendo la informaci√≥n agregada de la base original a cada miembro de un partido. Adem√°s se generaron nuevas variables x, y, row, theta que marcan la posici√≥n en que se graficar√° cada una de las bancas.\n\nPor √∫ltimo una secuencia de c√≥digo con la que realizamos el gr√°fico. Notar que las geometr√≠as que especificamos a ggplot son del tipo gen√©rico geom_parliament_. Cada una agrega una capa:\n\n\nCon geom_parliament_seats podemos manipular el tama√±o del punto que representa un esca√±o.\nEn geom_parliament_goverment estamos especificando par√°metros para resaltar cu√°les esca√±os corresponden al partido de gobierno.\nCon geom_parliament_bar incorporamos la barra superior que agrega el \\(\\%\\) de bancas de cada espacio.\n\nPor √∫ltimo, con draw_majoritythreshold podemos marcar la posici√≥n que permite distinguir si hay un espacio pol√≠tico que alcanza la mayor√≠a\n\nlibrary(tidyverse)\nlibrary(ggparliament)\nlibrary(ggthemes)\n\n\ndata_diputados_tucuman %&gt;% as_tibble() %&gt;% \n  ggplot(aes(x, y, colour = party_long)) +\n  geom_parliament_seats(size = 7) + # tama√±o de bancas (puntos)\n  geom_highlight_government(government == 1, colour = \"black\", size = 8) + # circulo negro al oficialismo\n  geom_parliament_bar(party = party_short, label = F) + # barra con proporci√≥n de bancas\n  draw_majoritythreshold(n = 31, label = F, type = \"semicircle\") + # dibuja el limite de mayor√≠a \n  scale_colour_manual(values = data_diputados_tucuman$colour, #asigno colores \n                      limits = data_diputados_tucuman$party_long)  +\n  guides(colour = guide_legend(nrow=7)) + # customiza etiquetas\n  labs(title = \"TUCUM√ÅN\", \n       subtitle = \"Diputados 2019 - 2021\",  \n       colour = \"Bloques\") +\n  theme_fivethirtyeight() + # est√©tica de gr√°ficos estilo FiveThiryEight\n  theme(panel.grid = element_blank(), \n        axis.text = element_blank(), \n        legend.position = \"bottom\", \n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\nSer√° hasta la pr√≥xima entrada."
  },
  {
    "objectID": "posts/2020-03-15-dibujar-legislaturas.html#footnotes",
    "href": "posts/2020-03-15-dibujar-legislaturas.html#footnotes",
    "title": "dibujaR legislatuRas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAc√° puede consultarse el repositorio oficial de la liberer√≠a con m√°s ejemplos y definiciones https://github.com/RobWHickman/ggparliament‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2020-03-11-los-discursos-de-les-presidentes.html",
    "href": "posts/2020-03-11-los-discursos-de-les-presidentes.html",
    "title": "Los discursos de les presidentes",
    "section": "",
    "text": "(Juan Pablo Ruiz Nicolini, Iv√°n Lewin y Camila Higa1)\nTodos los 1 de marzo, ambas C√°maras del Congreso se re√∫nen para dar comienzo al a√±o legislativo y cada a√±o el presidente de turno encabeza el acto de Apertura. Los discursos2 suelen girar en torno a los ejes de gobierno, las promesas y los objetivos del a√±o, pero m√°s all√° del contenido, cada uno tiene un estilo personal marcado por el presidente. Entender qu√© dicen y c√≥mo lo dicen nos motiv√≥ a hacer un an√°lisis sobre los 37 discursos desde el retorno de la Democracia.\nDuraci√≥n de los discursos a trav√©s del tiempo\nLos a√±os pasan y los discursos cambian. No solo var√≠a qu√© se dice, sino tambi√©n cu√°nto se dice. A trav√©s de una serie de tiempo, es posible observar c√≥mo cambia la duraci√≥n del discurso de a√±o a a√±o. Para una mejor visualizaci√≥n, se le asign√≥ a cada presidente un color y se abreviaron los nombres a sus iniciales3.\nEl promedio de cantidad de palabras utilizadas por discurso fue de 9880. Sin embargo, se observan diferencias marcadas entre presidentes. CSM, FDR, ED y MM se mantuvieron siempre muy por debajo del promedio, mientras que NK y CFK se mantuvieron cerca o lo superaron.\nCon 2846 palabras, el discurso de CSM en 1996 fue el m√°s corto, mientras que con 26189 palabras en 2013, CFK se lleva el r√©cord del m√°s largo (m√°s del doble que el promedio). Incluso, en √©sta √∫ltima, se observa una tendencia creciente a trav√©s de los a√±os (excepto en 2014). Por √∫ltimo, el discurso de este a√±o encabezado por AF tuvo la misma duraci√≥n que el promedio, m√°s largo que que los de su antecesor, pero a√∫n por debajo del promedio de los de NK y CFK.\nCu√°les son las palabras qu√© m√°s repiten les presidentes fue una de las primeras preguntas que comenzamos a hacernos. Una forma gr√°fica de explorarlo es a trav√©s de una nube de palabras comparativa en la que los t√©rminos que m√°s veces se mencionan tienen mayor centralidad y son de mayor tama√±o.\nEn general, los t√©rminos m√°s frecuentes suelen marcar cu√°les son los ejes m√°s importantes del discurso. Por ejemplo, la palabra que m√°s resalta en RA es ‚Äúdemocracia‚Äù. En el discurso de este a√±o se destacan mayormente ‚Äúdesarrollo‚Äù, ‚Äúdeuda‚Äù y ‚Äúfederal‚Äù. Pero tambi√©n aparecen palabras vinculadas a la cuesti√≥n de g√©nero que no aparecen frecuentemente en los discursos de sus antecesores: ‚ÄúLa situaci√≥n de las mujeres gestantes en Argentina presenta aspectos diversos. Distintos son los desaf√≠os que enfrenta la mujer que desea tener a su hijo de aquellas que asumen que deciden interrumpir su embarazo. Un Estado que cuida debe acompa√±ar a todas las mujeres para que esos procesos se desarrollen accediendo plenamente al sistema de salud.‚Äù\nFrecuencia de palabras\nEn el siguiente gr√°fico se pueden comparar las palabras m√°s repetidas por les presidentes excluyendo palabras muy frecuentes como art√≠culos, preposiciones y otras.\nOtro punto interesante para analizar es entender cu√°les son las palabras que son importantes en cada discurso. ¬øC√≥mo hicimos esto? A trav√©s del c√°lculo de TF-IDF (term frequency-inverse term frequency) que consiste en otorgarle menos peso a aquellas palabras que son muy comunes y darle m√°s peso a aquellas que son menos usadas a lo largo de todos los discursos.\nEntonces, a trav√©s de estos gr√°ficos podemos ver qu√© palabras se destacan en cada uno de los discursos. Por ejemplo, en el de AF se destaca ‚Äúsostenible‚Äù y aparecen entre las destacadas ‚Äúg√©nero‚Äù y ‚Äúembarazo‚Äù, mientras que, en los discursos de MM, las palabras var√≠an pero en los √∫ltimos tres a√±os, entre las palabras m√°s destacadas aparece alguna relacionada a obras (‚Äúcloacas‚Äù, ‚Äúparques‚Äù, ‚Äúpuentes‚Äù).\nDecidimos hacer el mismo ejercicio, pero esta vez con bigramas (frecuencia de dos palabras consecutivas). En el gr√°fico de abajo se pueden visualizar los bigramas m√°s frecuentes por discurso:\nEl contexto cambia a√±o a a√±o y los discursos tambi√©n. Los bigramas m√°s frecuentes no suelen repetirse ni siquiera a lo largo de un mismo per√≠odo presidencial. Claro que hay algunas excepciones, por ejemplo, en los cuatro discursos de NK aparece ‚Äúderechos humanos‚Äù entre el top 5 de bigramas: ‚ÄúUn pa√≠s con memoria, verdad y justicia tiene que comprometerse profundamente con la defensa de los derechos del hombre (‚Ä¶). Desde un punto al otro del espectro ideol√≥gico, la defensa de los derechos humanos debe constituir un compromiso nacional y racional.‚Äù(2004)\nEn el gr√°fico de abajo se pueden observar los bigramas calculados con TF-IDF:\nSemejanzas y diferencias\nYa dijimos que cada presidente elige sus palabras y tiene su estilo, pero nos propusimos ver qu√© tanto se acercan los discursos entre s√≠. Para esto realizamos un An√°lisis de Componentes Principales (PCA)4. La idea es que mediante esta t√©cnica estad√≠stica, considerando la frecuencia de palabras, se puede ubicar cada uno de los discursos en un plano de acuerdo al contenido de los mismos.\nAlgunas observaciones preliminares del gr√°fico en relaci√≥n a los discursos:\n‚Ä¢ En general, los discursos por presidente se encuentran cercanos entre s√≠ (a excepci√≥n del de 1998 de CSM y de los de FDR y ED).\n‚Ä¢ Los de MM y de CFK se encuentran alejados del resto.\n‚Ä¢ Los de los primeros dos a√±os y de los dos √∫ltimos de MM fueron mucho m√°s cercanos entre s√≠.\n‚Ä¢ El de AF se posiciona m√°s cerca de los de NK y, por lo tanto, alejado de sus antecesores inmediatos.\nAl explorar los datos, tanto a trav√©s de las frecuencias de palabras como del modelado de t√≥picos, dos grandes ejes parecen ser capaces de englobar la mayor√≠a de los t√©rminos expresados en los discursos. Dada esta intuici√≥n e inspirados en un estudio de la Universidad de Berkeley5 tomamos los primeros discursos de cada uno de los mandatos presidenciales para clasificar las palabras m√°s frecuentes en cada una de estas categor√≠as o ‚Äút√≥picos‚Äù.\nSi ponemos el foco sobre los t√©rminos de pol√≠tica utilizados m√°s frecuentemente (representado por el tama√±o de las etiquetas del lado derecho) se destacan ‚Äújusticia‚Äù, ‚Äúnaci√≥n‚Äù y ‚Äúpueblo‚Äù por sobre ‚Äúlibertad‚Äù, ‚Äúderecho‚Äù y ‚Äúdi√°logo‚Äù. Y al analizar las palabras relacionadas al eje econom√≠a resaltan ‚Äúcrecimiento‚Äù y ‚Äúdesarrollo‚Äù por sobre el resto.\nOtra manera de visualizar la informaci√≥n puede ser observar la frecuencia con que los distintos discursos utilizaron alguno de los t√©rminos de cada t√≥pico. Ello est√° representado por el ancho de los flujos de colores. As√≠, por ejemplo, se observa que las intervenciones en el primer discurso de MM se restringen solo a tres de los t√©rminos: ‚Äúderecho‚Äù, ‚Äúdi√°logo‚Äù y ‚Äújusticia‚Äù en el t√≥pico pol√≠tica. O que las palabras m√°s frecuentes de AF en el que refiere a econom√≠a fueron ‚Äúdeuda‚Äù y ‚Äúdesarrollo‚Äù.\nPor √∫ltimo, calculamos que porcentaje del discurso representa la sumatoria de las palabras de cada uno de los ejes (representados en los c√≠rculos que refieren a cada uno de ellos). As√≠, por ejemplo, se observa que los primeros discursos de FDR Y CSM fueron los que usaron m√°s frecuentemente palabras vinculadas a pol√≠tica como proporci√≥n de sus intervenciones, mientras que el de MM fue el que tuvo la menor cantidad de palabras vinculadas a este t√≥pico. Por otro lado, √©ste √∫ltimo es el que utiliz√≥ proporcionalmente m√°s veces palabras relacionadas a econom√≠a, mientras que CSM fue el que menos las utiliz√≥.\n(Todas las visualizaciones pueden descargarse de este repositorio)"
  },
  {
    "objectID": "posts/2020-03-11-los-discursos-de-les-presidentes.html#footnotes",
    "href": "posts/2020-03-11-los-discursos-de-les-presidentes.html#footnotes",
    "title": "Los discursos de les presidentes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPublicado originalmente en https://mentacomunicacion.github.io/‚Ü©Ô∏é\nLos discursos fueron compilados por Antonio Milanese a partir de lo publicado en la HCDN.‚Ü©Ô∏é\nRA = Ra√∫l Alfons√≠n, CSM = Carlos Sa√∫l Menem, FDR = Fernando de la R√∫a, ED = Eduardo Duhalde, NK = N√©stor Kirchner, CFK= Cristina Fern√°ndez de Kirchner, MM = Mauricio Macri, AF = Alberto Fern√°ndez‚Ü©Ô∏é\nUn resultado similar obtuvimos al modelar los t√≥picos contenidos en los discursos. Preferimos mostrar esta distribuci√≥n en un plano de dos dimensiones en funcion del PCA porque resulta visualmente m√°s inteligible.‚Ü©Ô∏é\nhttps://datascience.berkeley.edu/blog/trump-state-of-the-union-analysis/‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2019-09-08-latinr-2019.html",
    "href": "posts/2019-09-08-latinr-2019.html",
    "title": "LatinR 2019‚Ä¶ all√° vamos.",
    "section": "",
    "text": "Los pr√≥ximos 25, 26 y 27 de septiembre se desarrollar√° en Santiago de Chile la segunda edici√≥n de LatinR: Conferencia Latinoamericana sobre el uso de R en I+D. Tuve la suerte de participar en la primera edici√≥n, que se llev√≥ a cabo en Buenos Aires hace un a√±o, donde pude mostrar un peque√±o proyecto -a√∫n en desarrollo- que consiste en el armado de grillas como si fueran mapas (geofacet) aplicados a divisiones pol√≠ticas de Argentina (a nivel pa√≠s y provincial) 1.\nGracias al apoyo del comit√© evaluador tendr√© la oportunidad de participar nuevamente en esta segunda edici√≥n. En esta ocasi√≥n mostrar√© un proyecto en el que trabajamos con el colega y tocayo JP Pilorget para menta Comunicaci√≥n . Un dashboard de Shiny que permite la visualizaci√≥n r√°pida y centralizada de variables electorales, de inter√©s para el an√°lisis pol√≠tico y el desarrollo de campa√±as electorales (en desarrollo).\nUn proyecto que deriv√≥ de Inteligencia Electoral pero termin√≥ como un producto paralelo fue el √çndice Socioecon√≥mico menta, que puede consultarse en l√≠nea ac√°.\nTodo el flujo de trabajo de estos proyectos fue realizado con R, desde el entorno de RStudio:\nHabiendo pasado los avisos parroquiales, quiero aprovechar para agradecer nuevamente a todo el equipo detr√°s de LatinR, sus chairs, comit√©s y equipo de coordincaci√≥n. Si la experiencia de 2018 y el contacto (sobre todo ciebrn√©tico, por la red del pajarito y slack) desde entonces fue para m√≠ un lugar de gran aprendizaje, no dudo que la edici√≥n de 2019 llevar√° eso a otro nivel.\nPara quienes puedan tener inter√©s, ac√° pueden acceder al programa, con detalle de las mesas, ponencias y talleres, todos muy prometedores. Por mi cuenta, mientras que en 2018 pude tener mi primer acercamiento a purrr de la mano de Jenny Bryan, ahora estoy entusiasmado por sumergirme en el desarrollo de paquetes con el tutorial de H. Wickham. Botones de muestra del gran laburo de quienes empujan por m√°s R nuestroamericano."
  },
  {
    "objectID": "posts/2019-09-08-latinr-2019.html#footnotes",
    "href": "posts/2019-09-08-latinr-2019.html#footnotes",
    "title": "LatinR 2019‚Ä¶ all√° vamos.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEsto dec√≠amos en un post previo: geofaceteAR‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TuQmano",
    "section": "",
    "text": "argendata: El camino de los datos\n\n\nC√≥mo se construye una infraestructura de datos abiertos con foco en evidencia, reproducibilidad y colaboraci√≥n\n\n\n\nargendata\n\n\ndata\n\n\nfundar\n\n\n\n\n\n\n\n\n\nJul 22, 2025\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nSe va la segunda‚Ä¶\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nUn 2020 con mucho amoR\n\n\n\n\n\n\nLatinR\n\n\nr-pkg\n\n\nShiny\n\n\nTeaching\n\n\n\n\n\n\n\n\n\nDec 30, 2020\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nLatinR in quarantine\n\n\n\n\n\n\nLatinR\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\nTuQmano\n\n\n\n\n\n\n\n\n\n\n\n\npolaR on CRAN\n\n\n\n\n\n\nR\n\n\nCRAN\n\n\nr-pkg\n\n\n\n\n\n\n\n\n\nMay 22, 2020\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nDatos y Covid19\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\n\n\nMar 29, 2020\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\ndibujaR legislatuRas\n\n\n\n\n\n\nViz\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nMar 15, 2020\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nLos discursos de les presidentes\n\n\n\n\n\n\nText Minning\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nMar 11, 2020\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nCongreso 2.0: politicos argentinos en Twitter\n\n\n\n\n\n\nTwitter\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nDec 20, 2019\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nLatinR2019 ha muerto‚Ä¶ que viva LatinR2020\n\n\n\n\n\n\nLatinR\n\n\n\n\n\n\n\n\n\nSep 30, 2019\n\n\nTuQmano\n\n\n\n\n\n\n\n\n\n\n\n\nLatinR 2019‚Ä¶ all√° vamos.\n\n\n\n\n\n\nLatinR\n\n\nViz\n\n\n\n\n\n\n\n\n\nSep 8, 2019\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\ndiagRamas\n\n\n\n\n\n\nViz\n\n\n\n\n\n\n\n\n\nApr 18, 2019\n\n\nTuQmano\n\n\n\n\n\n\n\n\n\n\n\n\nGeofaceteAR\n\n\n\n\n\n\nR\n\n\nViz\n\n\npolAr\n\n\ngeofacetAr\n\n\nr-pkg\n\n\n\n\n\n\n\n\n\nJan 21, 2019\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\n\n\n\n\n\n\nBienvenidos a TuQmanoR\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 17, 2019\n\n\nJuan Pablo Ruiz Nicolini\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "El TuQmano",
    "section": "",
    "text": "Juan Pablo Ruiz Nicolini\nTambi√©n conocido como el TuQmano. Soy polit√≥logo\\(^2\\) por la Universidad Torcuato Di Tella. Trabajo con datos y visualizaciones. Me gustan las elecciones. Cientista de Datos en progreso (DS). Director del √°rea de Datos en Fundar y coordinador de datos en argendata\n\n\nCorr√≠a el a√±o 2015, un estudiante de maestr√≠a de Ciencia Pol√≠tica ten√≠a que procesar datos para entregar su t√©sis y decidi√≥ aprovechar la oportunidad para aprender a usar R. Al largo y tortuoso camino del requisito acad√©mico se agreg√≥ un grado de dificultad extra: incorporar un software de programaci√≥n desde cero, con una curva de aprendizaje algo empinada.\n\n\nA√±os despu√©s reviso scripts del c√≥digo de la tesis y me sonr√≠o al ver extensas l√≠neas de c√≥digo para recodificar variables.\n\n\nMirando en restropectiva, como a mis scripts, aprender R fue la mejor decisi√≥n que tom√© en mi carrera profesional.\n\n\nR desde entonces en una herramienta fundamental de mi d√≠a a d√≠a. Lo es en el √°mbito acad√©mico y como docente universitario; en mis trabajos como consultor y analista; en mis tareas como funcionario p√∫blico; en mis ratos libres, como un pasatiempo; y en mi trabajo actual, coordinando el equipo de Datos de Fundar, un think tank que estudia y elabora pol√≠ticas p√∫blicas para el desarrollo sostenible e inclusivo de Argentina.\n\n(Extracto de Divide y venceras: de polar al Polarverse)\n\nComo polit√≥logo mis temas de investigaci√≥n se concentraron en estudios electorales, partidos y reforma pol√≠tica. En particular, he trabajado en la evaluaci√≥n del impacto pol√≠tico del cambio en la tecnolog√≠a con la que se emiten los votos.\nComo DS vengo trabajando en el desarrollo de paquetes de R para facilitar herramientas y el acceso a datos pol√≠ticos de Argentina."
  },
  {
    "objectID": "posts/2019-04-18-diagramas.html",
    "href": "posts/2019-04-18-diagramas.html",
    "title": "diagRamas",
    "section": "",
    "text": "Ayer particip√© en el segundo #Mi√©rcolesDeDatos, que replica el desaf√≠o semanal #TidyTuesday de su hermano angl√≥fono por iniciativa de @R4DS_es, quienes colaboran para hacer accesible la biblia de la comunidad datera de R para la comunidad hispanoparlante.\nEl proyecto de ayer consit√≠a en hacer un an√°lisis de datos y visualizaci√≥n con informaci√≥n de los libros de Game of Thrones. Esto fue lo que compart√≠\nAprovech√© los datos de este #Mi√©rcolesDeDatos para probar una de las features que incluye ggforce (el acelerador de ggplot creado por Thomas Lin Pedersen).\nQu√© tipo de gr√°fico es √©ste?\nEl mismo Pedersen (cientista de datos en RStudio - @thomasp85 en Twitter) se√±ala que hay una familia de estas figuras con nombres distintos. Y destaca cuales son para √©l las diferencias y particularidades de cada una de esas nomenclaturas (Sankey, Alluvial y Parrallel Sets).\nEn sus palabras (traducidas) ser√≠a algo as√≠:\n\nHay un poco de confusi√≥n en la nomenclatura con esto. Insistir√© en que los diagramas de Sankey son espec√≠ficamente para flujos (y, a menudo, emplean una posici√≥n m√°s suelta de los ejes) y los diagramas aluviales son para seguir los cambios temporales, pero todos podemos ser amigos, no importa c√≥mo se llame.\n\n\nYa hace alg√∫n tiempo empec√© a jugar con gr√°ficos de este tipo. Se me ocurren r√°pido algunos ejemplos:\n\nSANKEY\nEn primer lugar unos gr√°ficos que fueron producidos con SankeyMatic (una recomendaci√≥n de Crst_C).\n\n(El ejercicio buscaba ilustrar el origen partidario de los votos de una alianza. El an√°lisis completo ac√°).\n\n\nggalluvial\nEn segundo lugar intent√© rehacer un gr√°fico hecho con SankeyMatic desde R, procurando reproducibilidad. Ac√° el original, abajo la versi√≥n reproducible:\n\nEl ejericio buscaba ilustrar como se distribuyeron las fuentes de finanicmiento privada de partidos pol√≠ticos en la elecci√≥n nacional de 2017 en la Provincia de Buenos Aires. El an√°lisis completo ac√°.\n\n\nParrallel Sets\nAyer complet√© el trio con el ejemplo de GoT usando el geom_parrallel_sets de ggforce. Respecto de esta variante Pedersen agrega que:\n\nEl problema principal es que los datos para los gr√°ficos de conjuntos paralelos generalmente no se representan muy bien en el formato tidy esperado por ggplot2, por lo que ggforce proporciona una funci√≥n para obtener con ese formato.\n\nEsta funci√≥n amiga es gather_set_data(a:b), la cual selecciona las variables que queremos incluir en nuestros ejes. Genera nuevas variables llamadas id, x, y. Junto con value - el conteo de personajes por cada uno de los grupos que definimos- y Lealtad, que es la variable que usamos para definir los colores de las casas de GoT, tenemos toda la informaci√≥n.\nEl c√≥digo completo y el plot son los siguiente:\n\nlibrary(tidyverse)\nlibrary(ggforce)\nlibrary(ggthemes)\n\n#GoT PLOT - #Mi√©rcolesDeDatos\nreadr::read_csv(\"https://raw.githubusercontent.com/cienciadedatos/datos-de-miercoles/master/datos/2019/2019-04-17/personajes_libro.csv\") %&gt;%  #CARGAMOS DATA SET\n  select(2:4) %&gt;%  # SELECCIONAMOS COLUMNAS CON DATOS QUE VAMOS A USAR\n  group_by(lealtad, genero, noble) %&gt;%   # AGRUPAMOS POR TRES VARIABLES\n  mutate(value = n()) %&gt;%  # CONTAMOS CUANTOS PERSONAJES HAY POR GRUPO\n  ungroup() %&gt;% \n  mutate(noble = ifelse(noble == 1, \"Noble\", \"Plebeyo\"), \n         genero = ifelse(genero == \"masculino\", \"H\", \"M\")) %&gt;% \n  rename('Genero' = 'genero', \n         'Status' = 'noble', \n         'Lealtad' = 'lealtad') %&gt;% \n  gather_set_data(2:3) %&gt;%  # TRANSFORMACION TIDY DE PARRALLEL SETS (ggforce)\nggplot(aes(x, id = id, split = y, value = value)) + #  INICIA GRAFICO\n  geom_parallel_sets(aes(fill = Lealtad), alpha = 0.5, axis.width = 0.1) +\n  geom_parallel_sets_axes(axis.width = 0.1) +\n  geom_parallel_sets_labels(colour = 'white') +\n  theme_fivethirtyeight() +\n  theme(axis.text.y = element_blank()) +\n  labs(title = \"Genero y Status de personajes de GoT\", \n       subtitle = \"#DatosDeMi√©rcoles (por @TuQmano)\", \n       caption = \"Datos de @R4DS_es\")"
  },
  {
    "objectID": "posts/2019-09-30-latinr2019-ha-muerto-que-viva-latinr2020.html",
    "href": "posts/2019-09-30-latinr2019-ha-muerto-que-viva-latinr2020.html",
    "title": "LatinR2019 ha muerto‚Ä¶ que viva LatinR2020",
    "section": "",
    "text": "Termin√≥ nom√°s la segunda edici√≥n de #LatinR. Tres d√≠as a pleno con workshops, exposiciones plenarias, presentaciones y posters. Diversidad de disciplinas, participantes de casi toda nuestramerica e invitados super grosos. Donde no solo aprend√≠ cuestiones t√©cnicas y nuevas herramientas, sino tambi√©n el ejericio constante de reforzar el trabajo colaborativo dentro de una comunidad diversa y plural.\nYo en particular tuve la gran oportunidad adem√°s de (a) participar de un genial taller sobre el desarrollo de paquetes, con el messi de este l√≠o (H.Wickham - arriba); y (b) mostrar parte del trabajo y aprendizaje desde aquel ya lejano #LatinR2018.\nAc√° se puede acceder al shiny app de Inteligencia Electoral. Ac√° las slides de la presentaci√≥n y ac√° el abstract."
  },
  {
    "objectID": "posts/2019-09-30-latinr2019-ha-muerto-que-viva-latinr2020.html#latinr-en-la-red-del-pajarito",
    "href": "posts/2019-09-30-latinr2019-ha-muerto-que-viva-latinr2020.html#latinr-en-la-red-del-pajarito",
    "title": "LatinR2019 ha muerto‚Ä¶ que viva LatinR2020",
    "section": "LatinR en la red del pajarito",
    "text": "LatinR en la red del pajarito\nTwitter es un gran lugar para estar conectado con las novedades de #rstats. Y LatinR nos di√≥ la posibilidad de poder ponerle cara a un mont√≥n de @rrobas con los que uno interact√∫a cotidianamente en esa conexi√≥n. Algo de todo esto se pudo vivir a trav√©s de twitter detr√°s del hashtag #LatinR2019. Los gr√°ficos de abajo son una muestra de eso.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZoom al grafo.\nYa pasaron Buenos Aires (2018) y Santiago (2019). Llega el turno de LatinR2020 en Montevideo."
  },
  {
    "objectID": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html",
    "href": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html",
    "title": "Congreso 2.0: politicos argentinos en Twitter",
    "section": "",
    "text": "(Juan Pablo Ruiz Nicolini, Camila Higa, Franco Galeano y Carolina Rossi1)\nLa conformaci√≥n de un nuevo cuerpo legislativo para los pr√≥ximos dos a√±os en Argentina nos motiv√≥ a hacer un an√°lisis de la estructura, funcionamiento e interacciones de las cuentas de Twitter de los legisladores nacionales que conformar√°n el Congreso de la Naci√≥n durante el periodo 2019 - 2021.\nTal como hici√©ramos respecto a las cuentas de legisladores pertenecientes al Senado de la Naci√≥n (disponible ac√°), en los siguientes apartados nos proponemos hacer una radiograf√≠a de la C√°mara de Diputados."
  },
  {
    "objectID": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#los-del-bloque-sean-unidos",
    "href": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#los-del-bloque-sean-unidos",
    "title": "Congreso 2.0: politicos argentinos en Twitter",
    "section": "Los del bloque sean unidos‚Ä¶",
    "text": "Los del bloque sean unidos‚Ä¶\n¬øC√≥mo se vinculan las cuentas de legisladores seg√∫n los espacios pol√≠ticos? Para dar una respuesta podemos indagar cu√°les de las cuentas de legisladores es seguida por sus pares en Twitter. De este modo es posible construir un grafo que nos permita describir la intensidad de los v√≠nculos. Esto es, la relaci√≥n de amistad entre miembros de las c√°maras.\n\n\n\n\n\n\n\n\n\n\nCada c√≠rculo corresponde a una cuenta de una diputada o diputado (nodo).\nLas flechas (aristas) marcan la direcci√≥n de la conexi√≥n. Esto es, qui√©n sigue a qui√©n.\nLos colores son asignados luego de haber agrupado cada una de las cuentas con un m√©todo de clusterizaci√≥n random walk, el cual ordena las cuentas en funci√≥n de las relaciones entre las mismas definiendo comunidades.\n\nPrevisiblemente se visualiza una mayor intensidad de conexiones entre miembros de cada uno de los espacios pol√≠ticos: mayor cantidad de conexiones azules entre miembros del Frente de Todos y mayor cantidad de conexiones amarillas entre los de la oposici√≥n.\nUna versi√≥n interactiva nos permite seleccionar a los distintos usuarios para visualizar la red que conforman sus relaciones individuales. Tomemos como ejemplo los v√≠nculos que se generan con la cuenta ljuez como nodo de referencia. All√≠ se observa que casi la totalidad de sus v√≠nculos se dan con el cluster m√°s afin a su espacio pol√≠tico salvo con una √∫nica cuenta del oficialismo: sergiomassa.\n\n\n\n\n\n\n\n\n\n\nTres diputadas son seguidas por solo un colega: MelinaDelu, Adriana_Ruarte y graciela_navarr.\nCon \\(119\\) colegas que lo siguen, sergiomassa lidera el ranking. Esto tambi√©n es cierto cuando se restringe la cuenta para analizar solamente qui√©n es el m√°s seguido por opositores (\\(65\\)). Si calculamos en cambio qui√©n es m√°s seguido √∫nicamente por miembros de un mismo espacio marioraulnegri lidera el ranking (\\(88\\) seguidores).\n\\(50\\) de las cuentas siguen a menos de \\(10\\) de sus pares.\n\nsergiomassa es tambi√©n el m√°s seguidor con \\(146\\) de sus pares entre sus follows. Lo secunda _alvarogonzalez con \\(98\\)."
  },
  {
    "objectID": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#ecosistemas-pol√≠ticos-en-twitter",
    "href": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#ecosistemas-pol√≠ticos-en-twitter",
    "title": "Congreso 2.0: politicos argentinos en Twitter",
    "section": "Ecosistemas pol√≠ticos en Twitter",
    "text": "Ecosistemas pol√≠ticos en Twitter\nM√°s all√° del v√≠nculo espec√≠fico entre miembros de la C√°mara de Diputados, la informaci√≥n recolectada nos permite avanzar sobre la totalidad de las cuentas. Como veremos a continuaci√≥n, al analizar el listado de seguidos/amigos de cada legislador podemos hechar luz sobre sus posicionamientos pol√≠ticos con informaci√≥n sobre sus conexiones que excede los l√≠mites de la C√°mara. Sabemos que la suma de amigos de diputados alcanza unas \\(123_{mil}\\) cuentas distintas. Pero, ¬øcu√°les son m√°s representativas entre los seguidos por diputados? ¬øQu√© diferencias se pueden encontrar seg√∫n el alineamiento pol√≠tico?\nCuentas m√°s seguidas\nEl gr√°fico que sigue muestra las 30 cuentas m√°s seguidas por diputados argentinos. Entre ellas se destacan cuentas institucionales (@DiputadosAR, @CasaRosada y @SenadoArgentina); de medios (@LANACION, @infobae, @AgenciaTelam, @todonoticias y @perfilcom); de periodistas (@Gatosylvestrey @majulluis) y de dirigentes pol√≠ticos (@mauriciomacri, @marquitospena, @CFKArgentina, @mariuvidal, y @alferdez, entre otros).\n\n\n\n\n\n\n\n\n\nLa diferencia de seguidos por espacio pol√≠tico en la C√°mara de Diputados son notorias para esta muestra. Solamente \\(4/30\\) cuentas analizadas son m√°s seguidas por legisladores pertenecientes al Frente de Todos (alferdez, CFKArgentina, danielscioli y Gatosylvestre). Las cuentas Pontifex_es y SergioMassa, en tanto, son las m√°s cercanas a una zona de empate. Al rev√©s, la mayor√≠a de las cuentas de medios y periodistas y, m√°s lejos, dirigentes de la oposici√≥n tienen un claro sesgo de seguidos a favor de diputadas y diputados de ese espacio pol√≠tico.\n\n\n\n\n\n\n\n\n\nPara profundizar al respecto nos propusimos en primer lugar replicar el an√°lisis de Michael Kearney sobre el 116¬∫ Congreso de Estados Unidos analizando las cuentas de les diputades de Argentina.\n\nCuentas seguidas por diputadas y diputados de Argentina (2019-2021)\n\n\n\n\n\n\n\n\n\nMientras el eje horizontal representa la cantidad de cuentas del oficialismo que siguen a cada uno de los m√°s de \\(123_{mil}\\) perfiles de la muestra, el eje vertical lo hace respecto de la oposici√≥n. Se destacan los nombres de perfil de todas las cuentas que tienen entre \\(60\\) y \\(75\\) seguidores seg√∫n los espacios pol√≠ticos. Estos fueron coloreados en  rojo (oposici√≥n) y azul (oficialismo) seg√∫n la predominancia de uno respecto del otro y se marc√≥ en verde la zona de empate (cuando la diferencia entre los dos fue menor a \\(20\\), un \\(8.5\\%\\) de las cuentas de diputados, aproximadamente).\n\nUn ejercicio alternativo fue poner el foco en la cuentas m√°s seguidas para cada espacio pol√≠tico. Para ello restringimos el ranking luego de ordenar los datos minimizando la cantidad de seguidores de cada uno de los espacios. Estas cuentas se encuentran m√°s pegadas a los ejes respectivos de cada ecosistema en el gr√°fico anterior. Las tablas que siguen rankean estas cuentas bajo \\(2\\) condiciones ordenando seg√∫n prevalencia de seguidores (i) oficialistas y (ii) opositores.\nAs√≠, cuando ponemos el foco sobre las cuentas m√°s seguidas por oficialistas, observamos entre las 10 principales a cuentas partidarias (FrenteDeTodos, p_justicialista , EquipoCFK y FpVNacional) o de dirigentes afines (Vsiley, MontenegroViki, dipwlatercorrea, vtolosapaz y jorgeferraresi) .\n\n\n\n\n\n\n\n\n\n+ Oficialistas\nOPOSICION\nOFICIALISMO\nTOTAL \n\n\n\n\nFrenteDeTodos\n1\n62\n63\n\n\np_justicialista\n1\n44\n45\n\n\nVsiley\n1\n42\n43\n\n\nMontenegroViki\n1\n39\n40\n\n\nEquipoCFK\n1\n37\n38\n\n\ndipwaltercorrea\n1\n35\n36\n\n\nFpVNacional\n1\n35\n36\n\n\nvtolosapaz\n1\n35\n36\n\n\njorgeferraresi\n1\n34\n35\n\n\nLuciMasin\n1\n34\n35\n\n\n\nLo mismo sucede si ponemos el foco en las cuentas m√°s seguidas por opositores, una vez que se minimiza la cantidad de oficialistas que siguen a esas mismas cuentas. Observamos en los 10 m√°s seguidos a dirigentes radicales (SalvadorPBA, alechegarayUCR) y del PRO (SchiavoniH, moralesfederico). Se destacan tambi√©n la cuenta institucional juntoscambioar.\n\n\n\n\n\n\n\n\n\n+ Opositores\nOPOSICION\nOFICIALISMO\nTOTAL \n\n\n\n\njuntoscambioar\n91\n1\n92\n\n\nSalvadorPBA\n49\n1\n50\n\n\ndante_sica\n44\n1\n45\n\n\nLuisBorsani\n42\n1\n43\n\n\nSchiavoniH\n42\n1\n43\n\n\nmoralesfederico\n38\n1\n39\n\n\nEduardoCostaSC\n37\n1\n38\n\n\nmetchecoin\n37\n1\n38\n\n\nalechegarayUCR\n36\n1\n37\n\n\nCalivillalonga\n34\n1\n35\n\n\n\n\n\nLos motivos para ‚Äúseguir‚Äù o ‚Äúser seguido‚Äù por un pol√≠tico son variados, y van desde la afinidad partidaria, cultural o territorial hasta el espionaje, pasando por el consumo ir√≥nico. Sin embargo, la intuici√≥n sugiere que la afinidad prevalece y la evidencia lo confirma: los pol√≠ticos siguen a m√°s ‚Äúamigos‚Äù que a rivales. Como la decisi√≥n de a qui√©n seguir es p√∫blica, con ella env√≠an se√±ales de pertenencia y de reciprocidad tanto a los votantes como a sus potenciales aliados. ‚Äî Calvo y Malamud"
  },
  {
    "objectID": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#amigos-son-los-amigos",
    "href": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#amigos-son-los-amigos",
    "title": "Congreso 2.0: politicos argentinos en Twitter",
    "section": "Amigos son los amigos",
    "text": "Amigos son los amigos\nCon la identificaci√≥n de todas esas cuentas construimos una matriz en la que cada fila se corresponde con un diputado y cada columna con cada uno de estos ‚Äúamigos‚Äù. En las celdas, en tanto, se asign√≥ un \\(1\\) cuando la cuenta de un legislador sigue a una cuenta y con un \\(0\\) cuando no lo hace.\nEl paso siguiente fue calcular c√≥mo correlaciona la lista de seguidos de cada una de las cuentas de legisladores comparada con sus pares. Visualmente puede analizarse de la siguiente manera:\n\nDIPUTADOS\n\n\n\n\n\n\n\n\n\nLos ejes listan cada una de las cuentas analizadas de la muestra de Diputados de la Naci√≥n (agrupadas por espacio pol√≠tico). La diagonal inferior (azul oscuro) muestra una correlaci√≥n positiva perfecta (\\(1\\)), dado que las observaciones corresponden a la misma cuenta en filas y columnas. As√≠, para el resto de las cuentas con las que se compara cada usuario, cuanto m√°s azul, la correlaci√≥n ser√° m√°s alta (los usuarios que siguen las cuentas son m√°s similares) y cuanto m√°s rojo, los ‚Äúamigos‚Äù de ese par de cuentas difieren m√°s.\nR√°pidamente se observa que en l√≠neas generales hay una asociaci√≥n positiva entre cuentas del Frente de Todos (el oficialismo, con las etiquetas en azul) y de la oposici√≥n (agrupadas con distintos colores por pertenencia de bloque dentro del interbloque Juntos por el Cambio u Otros).\nPor √∫ltimo, al comparar entre cuentas pertenecientes a polos distintos la tendencia es a la inexistencia de relaci√≥n (m√°s blanco, con valores cercanos a \\(0\\); o una asociaci√≥n negativa, con tonos m√°s rojos).\n\nEl siguiente gr√°fico ubica en un plano a los dos primeros componentes principales (que explican la mayor variabilidad de los datos de la matriz de usuarios seguidos por diputados)2. El gr√°fico ordena los @ en distintos espacios y destaca la pertenencia pol√≠tica de cada uno.\nLa parte superior agrupa la mayor√≠a de las cuentas del nuevo oficialismo bajo la etiqueta Frente de Todos. En la parte inferior, en tanto, se agrupan los usuarios identificados bajo el espacio Juntos por el Cambio, en el cual se destacan tambi√©n las distintas pertenencias pol√≠ticas originales (UCR, PRO, CC y Otros).\n\n\n\n\n\n\n\n\n\n(Figura en alta resoluci√≥n disponible ac√°)\nUna mirada m√°s detallada nos permite encontrar elementos salientes. Por ejemplo, considerando el espacio oficialista, se destacan la lejan√≠a de gallegopm, cbritezmisiones y juanmosqueda_ar del centro del Frente de Todos (ocupado por cuentas como pablocarrook y marabrawer).\n\n\n\n\n\n\n\n\n\nEn tanto, al observar el sub espacio de Juntos por el Cambio se observa una distribuci√≥n bastante centrada de la nube amarilla con miembros de las distintas fuerzas que lo componen (PRO, CC y UCR) con algunas cuentas m√°s alejadas como lucilalehmann, sebasnsalvador, alvarodlamadrid y dinarezi.\n\n\n\n\n\n\n\n\n\nLa mayor parte de las cuentas identificadas como Otros se ubican en una posici√≥n intermedia donde se cruzan las nubes de los dos espacios principales. All√≠ se observa por ejemplo a rominadelpla y nicolasdelcano del Frente de Izquierda y a miembros de bloques menores como paulocassinerio, almasapag y joseluisramon (m√°s aislado del resto de sus colegas)."
  },
  {
    "objectID": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#el-p√∫blico-siempre-tiene-la-raz√≥n",
    "href": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#el-p√∫blico-siempre-tiene-la-raz√≥n",
    "title": "Congreso 2.0: politicos argentinos en Twitter",
    "section": "El p√∫blico siempre tiene la raz√≥n",
    "text": "El p√∫blico siempre tiene la raz√≥n\nUn √∫ltimo ejercicio consisti√≥ en revisar c√≥mo interact√∫a el p√∫blico con las publicaciones emitidas desde las cuentas de diputados. Como ya desarrollamos en una entrada previa, les usuaries interact√∫an con el contenido de tres formas3:\n\nfav (me gusta - MG): la acci√≥n que menos exige / expone al que interact√∫a. La reacci√≥n m√°s com√∫n en Twitter.\nretweet (re publicaci√≥n - RT): mantiene la econom√≠a de esfuerzo del anterior, pero hace m√°s visible la interacci√≥n con el resto.\nreply (respuesta - RP): el tipo de reacci√≥n que implica mayor esfuerzo al tener que elaborar una respuesta.\n\n‚ÄúSi tuviste m√°s respuestas a un tuit que favs o RT, entonces la cagaste‚Äù. Cuando el RATIO (originalmente descripto ac√°) toma valores altos es indicador de que la recepci√≥n del tuit fue muy mala.\n\\[RATIO = \\frac{Respuestas}{Favs + RT} *100\\]\nLa forma de graficarlo para visualizarlo es tambi√©n simple a partir de gr√°ficos ternarios. Estos muestran la relaci√≥n entre las tres variables (retweet, fav y respuesta), donde los v√©rtices del tri√°ngulo representan a cada una de ellas. Cuanto m√°s se acerca un punto (un tweet) a un v√©rtice, mayor es el valor de esa variable respecto de las otras. Por ende, cuanto m√°s se expande la nube de puntos de la pared derecha (segmento que une los fav y rt) m√°s alto ser√° el valor del RATIO.\n\n\n\n\n\n\n\n\n\nEl gr√°fico anterior muestra el ratio promedio para cada una de las \\(233\\) cuentas de la muestra, coloreadas seg√∫n el espacio pol√≠tico de pertenencia. Los valores son relativamente bajos para la mayor√≠a de las cuentas. En la versi√≥n interactiva es posible explorar a la totalidad de los tuits emitidos por cada una de las cuentas junto al c√°lculo del ratio correspondiente a lo largo del tiempo."
  },
  {
    "objectID": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#footnotes",
    "href": "posts/2020-03-12-congreso-2-0-politicos-argentinos-en-twitter.html#footnotes",
    "title": "Congreso 2.0: politicos argentinos en Twitter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOriginalmente publciado en https://mentacomunicacion.github.io/‚Ü©Ô∏é\nT√©cnica estad√≠stica que consiste en obtener la mayor cantidad de informaci√≥n posible de un conjunto de datos construyendo variables latentes a partir de las variables existentes en la que se observan correlaciones y reducir la dimensionalidad del fen√≥meno. Las componentes son combinaciones lineales de las variables originales y se espera que solo unas pocas recojan la mayor parte de la variabilidad de los datos.‚Ü©Ô∏é\nExiste otra acci√≥n posible - citar tweet - pero se entiende que esta es una nueva intervenci√≥n sobre una publicaci√≥n existente y no simplemente una reacci√≥n.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2020-03-29-datos-y-covid19.html",
    "href": "posts/2020-03-29-datos-y-covid19.html",
    "title": "Datos y Covid19",
    "section": "",
    "text": "Hace unos poco d√≠as Andr√©s V√°zquez me invit√≥ junto a un conjunto de personas (especialistas en datos abiertos, cientistas de datos, economistas, inform√°ticos y desarrolladores de software) a responder \\(3\\) preguntas que tienen que ver con los desaf√≠os de la apertura de datos en el contexto de la cr√≠sis de la pandemia del Covid19.\nCadena de Datos / Episodio 34: En este contexto de crisis y preocupaci√≥n por la pandemia donde todos queremos saber que paso ahora, que tan grave es y que va a pasar en las pr√≥ximas semanas me surgen algunas dudas acerca de las pol√≠ticas de apertura y uso de los datos relacionados a este tema.\n\nMi reacci√≥n se centr√≥ en dos ejes: (1) la demanda de datos por parte del p√∫blico en general (como lo llam√≥ Diego Kozlowski en su propia intervenci√≥n) y (2) c√≥mo esto se relaciona con la mala informaci√≥n y la desinformaci√≥n.\n\nComo usuario aficionado a los datos p√∫blicos creo en la importancia del desarrollo y apertura de datos. Entiendo que una mayor disponibilidad de estos, en formatos en los que podamos reutilizarlos es saludable, al proveer a la comunidad de usuarios de datos insumos que favorecen un proceso de democratizaci√≥n del conocimiento. Existen experinecias que desde la sociedad civ√≠l han empujado el desarrollo de plataformas de datos abiertos en otros niveles. Aunque no son los √∫nicos, ejemplos como el GgastoPublicoBahiense o dat.ar son muestra de ello. Y, si bien falta, en Argentina ven√≠mos transitando un camino positivo. Aunque con ciertos d√©ficits, gobiernos locales, provinciales y dependencias del gobierno nacional han ido dando pasos en este sentido.\n\nAhora, creo que en el contexto de la pandemia esta virtud encuentra un l√≠mite m√°s restrictivo que lo usual en la capacidad y recursos que los estados tiene a mano para poder responder y que ellos deben ser destinados a resolver los cuellos de botella que la crisis implica.\n\nPor otro lado pienso que el uso de datos que muchas veces hacemos quienes conformamos la comunidad de usuarios debe hacerse con mayor responsabilidad. Sobre todo teniendo en cuenta los costos que pueden generar, no solo la (a) desinformaci√≥n ( ~ fake news), que en general es intencional, sino la (b) mala informaci√≥n o informaci√≥n erronea.\n\nPara ilustrar la primera podemos recurrir al pol√≠tico tuitero m√°s famoso del mundo:\n\n\n\n\n\n\n\n\n\nEse mapa que Trump comparti√≥ en medio de la discusi√≥n por el juicio pol√≠tico en el Congreso de EE.UU. buscaba contar una historia: la mayor√≠a de los noreteamericanos me apoya a m√≠. Muchos salieron a contestarle mostrando que esa manera de presentar la informaci√≥n era mentirosa. Otra mirada posible de los mismos datos (en los que se pone el foco en la densidad poblacional y no ya en los kilometros de extensi√≥n del territorio), y m√°s certera desde el punto de vista de lo que se estaba discutiendo, es representada en la siguiente ilustraci√≥n:\n\n\n\n\n\n\n\n\n\nPero, como mencionaba antes, no siempre se trata de la publicaci√≥n de informaci√≥n falsa intencionalmente. Muchas veces lo que puede suceder es que se distribuya informaci√≥n erronea o poco precisa, por la falta de conocimiento de dominioo metodolog√≠as particulares Y creo que, si bien esto sucede con muchos temas y √°reas de estudio, es particularmente sensible en un contexto cr√≠tico y espec√≠fico como el de la pandemia. Y que los costos potenciales asociados a la viralizaci√≥n de contenido erroneo es un tema sobre el que deber√≠amos tener especial atenci√≥n.\nCon ello no quiero decir que la comunidad de usuarios de datos no tenga un rol que cumplir; que deba mantenerse aislada de la discusi√≥n y que no haya espacio para aportes. Pero quiz√°s no sea √©ste el de analizar y compartir en sus redes sociales la evoluci√≥n epidemoiol√≥gica o de las consecuencias potenciales que puede tener el virus en salud. Se puede, por ejemplo, trabajar al servicio de otras √°reas que puedan requerir colaboraci√≥n por los efectos asociados a la cuarentena. Me valgo del ejemplo del trabajo que coordina el Data Sociologist Germ√°n Rosati, en el que se propusieron calcular el tiempo que demoran los habitantes de un radio censal en caminar hasta un cajero autom√°tico, un almac√©n o lugares similares. O tambien se puede colaborar brindando herramientas a partir de los datos con la que expertos en temas espec√≠ficos de la pandemia puedan ver facilitado sus an√°lisis. Pero, definitivamente, pienso que deber√≠amos limitar nuestro impet√∫ en descargarnos un dataset y generar una visualizaci√≥n bonita para compartir publicamente. Y en muchos casos no colaboran al mejor entendimiento de lo que nos sucede, sino que tiene efectos nocivos sobre la calidad de informaci√≥n que circula."
  },
  {
    "objectID": "posts/2020-10-10-latinr-in-quarantine.html",
    "href": "posts/2020-10-10-latinr-in-quarantine.html",
    "title": "LatinR in quarantine",
    "section": "",
    "text": "2018 en Buenos Aires. 2019 en Santiago de Chile. 2020 en ‚Ä¶ la virtualidad. La crisis desatada por la pandemia del Covid19 cambi√≥ el plan de realizar el tercer encuentro de la Conferencia Latinoamericana sobre Uso de R en Investigaci√≥n + Desarrollo en Montevideo (lugar donde se espera realizar el 4¬∞ encuentro).\nLatinR para m√≠ representa lo mejor de la comunidad de usuaRios: apertura, predisposici√≥n para ayudar y colaborar y diversidad son pilares sobre los que se apoya la conferencia. Ejemplo de ello result√≥ la reuni√≥n de muches para trabajar en conjunto propuestas para la rstudioconf::global(2021). Y creo tambi√©n que LatinR demostr√≥, por el gran trabajo de chairs, comit√© cient√≠fico y organizador, que se puede hacer una conferencia potenciando la diversidad y la apertura a nuevos usuarios, que en general tienen mayores restricciones de todo tipo para ser parte.\nEn t√©rminos personal√≠simos, volv√≠ a disfrutar mucho de la conferencia. No solo por haber podido mostrar parte del trabajo que desarrollamos con excelentes compa√±eres; tambi√©n por darme cuenta de que mucho de lo que nosotros pudimos avanzar se debe en buena medida al esfuerzo y trabajo de otres, de quienes aprendemos y nos inspiramos. Y saber que tenemos la posibilidad de contacto e intercambio flu√≠do con elles, es una de las grandes cosas de esta comunidad (les LatinR‚Äôs, RenBaires, eph‚Äôs, et al.)\n\n\n\n\n\n\n\n\n\n\n#LatinR2020\nEl contexto no impidi√≥ que se pudiera desarrollar una gran agenda, con t√≥picos diversos y participantes de muchos paises de nuestra Am√©rica Latina: Miner√≠a de Texto y Web Scrapping; Desarrollo de Paquetes; Datos Espaciales; Shiny y Visualizaci√≥n de Datos;Modelos y Aplicaciones y Aplicaciones con datos educativos y pol√≠ticas sociales.\nEn el marco del congreso se desarrollaron adem√°s una serie de talleres sobre Expresiones Regulares; desarrollo de Shiny Apps; uso de R Markdown; importaci√≥n de datos desde hojas de c√°lculo; ense√±anza en l√≠nea y tutoriales interactivos y machine learning.\nAlison Presmanes Hill (Rstudio) estuvo en la exposici√≥n inaugural contando qu√© es ‚ÄúAprender sin una red‚Äù. Ma√´lle Salmon expuso c√≥mo bloguear desde R Markdown.\nSe desarrollaron tambi√©n eventos asociados a la reproducibilidad de c√≥gido, manuscritos y art√≠culos (ReproHack) y un panel de accesibilidad e inclusi√≥n.\n\n2018 in Buenos Aires. 2019 in Santiago de Chile. 2020 in ‚Ä¶ virtuality. The crisis unleashed by the Covid19 pandemic changed the plan to hold the third meeting of the Latin American Conference on the Use of R in Research + Development in Montevideo (where the 4th meeting is expected to be held).\nLatinR for me represents the best of the useR‚Äôs community: openness, willingness to help and collaborate, and diversity are pillars on which the conference is based. An example of this was the meeting of many to work together proposals for the rstudioconf::global(2021). And I also believe that LatinR demonstrated, through the great work of the chairs, the scientific and organizing committee, that a conference can be held by promoting diversity and openness to new users, who in general have greater restrictions of all kinds to be part of.\n\n\n#LatinR2020\nThe context did not prevent a great agenda from being developed, with diverse topics and participants from many countries of our Latin America: Text Mining and Web Scrapping; Package Development; Spatial Data; Shiny and Data Visualization; Models and Applications; Applications with educational data and social policies.\nIn the framework of the congress, a series of workshops were also developed on Regular Expressions; Shiny Apps development; use of R Markdown; data import from spreadsheets; online teaching; interactive tutorials and machine learning.\nIn the oppening session Alison Presmanes Hill talked about what is ‚ÄúLearning without a net‚Äù. Ma√´lle Salmon showed us how to blog from R Markdown.\nEvents about reproducibility (ReproHack) and an accessibility and inclusion panel were also developed.\nIn very personal terms, I once again enjoyed the conference very much. Not only for being able to show part of the work that we develop with excellent colleagues; I also realized that much of what we were able to advance is largely due to the effort and work of others, from whom we learn and are inspired. And knowing that we have the possibility of having fluid contact and exchange with them is one of those great things that this community has (LatinR‚Äôs, RenBaires, eph‚Äôs, et al.).\n\n\n\n\n\n\n\n\n\n\n\n\nTuQmano @ #LatiR2020\n\n{polAr} - An√°lisis de Datos Pol√≠icos de Argentina\nAbstract | üìä Slides | üì¶ {polAr} | ‚å®Ô∏è blogpost | üìº Video | üì∫ Panel\n\n\nMinaR los discuRsos pResidenciales (with Camila Higa y Lucas Enrich)\nAbstract | üìä Slides | ‚å®Ô∏è blogpost | üìº Video | üì∫ Panel\n\n\n#Tuit√≥metroNacional: monitor de la conversaci√≥n pol√≠tica en Argentina (with Camila Higa).\nAbstract | üìä Slides | ‚å®Ô∏è blogpost | üìº Video | üíª App | üì∫ Panel"
  },
  {
    "objectID": "posts/2025-06-04_intro_argendata.html",
    "href": "posts/2025-06-04_intro_argendata.html",
    "title": "argendata: El camino de los datos",
    "section": "",
    "text": "En este, nuestro primer cumplea√±itos, quiero compartir un poco m√°s de detalle del laburo datero que hay por detr√°s de este peque√±o gran orgullo TuQmano. Y extender un agradecimiento por el enorme esfuerzo a todo fundar, pero en particular a ‚Äúlos datasets‚Äù, el equipo que lider√≥ el trabajo que ac√° comparto y a nuestra capitana Pau Isaak.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) El lanzamiento\n\n\n\n\n\n\n\n\n\n\n\n(b) Sprint final\n\n\n\n\n\n\n\n\n\n\n\n(c) El triunvirato\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Contando nuestra experiencia en Exactas (UBA)\n\n\n\n\n\n\n\n\n\n\n\n(e) Un d√≠a m√°s de sripting\n\n\n\n\n\n\n\n\n\n\n\n(f) Agotados, pero felices\n\n\n\n\n\n\n\nFigure¬†1: Un a√±o de datos en Argendata\n\n\n\n\n\n\n\nArgendata es una iniciativa de fundar que busca organizar, armonizar y publicar datos sobre Argentina con una l√≥gica clara: que sirvan para entender mejor, discutir mejor y decidir mejor. En definitiva, un peque√±o aporte que tiene una enorme ambici√≥n: transformar la Argentina.\nPero no se trata solo de subir textos y gr√°ficos. Detr√°s del sitio hay una estructura t√©cnica que permite mantener los datos vivos, trazables y disponibles. Una arquitectura pensada para que el contenido sea m√°s que informaci√≥n: que sea evidencia\n\n\nTL;DR\n\nEn Argendata, la calidad, transparencia y reproducibilidad de los datos son fundamentales para nutrir el debate p√∫blico con informaci√≥n confiable. Nuestros procesos est√°n dise√±ados para procurar que cada dato sea robusto y accesible. Nos basamos en tres pilares esenciales: la administraci√≥n de recursos, que asegura la coherencia y armonizaci√≥n de los datos; un riguroso control de calidad y reportes; y la armonizaci√≥n del c√≥digo fuente, clave para la reproducibilidad y actualizaci√≥n constante. Este enfoque nos permite transformar los datos generados por investigadores en las visualizaciones claras y √∫tiles que encontr√°s en nuestro sitio.\n\n\n\n\nPara lograr todo esto, en Argendata utilizamos un conjunto de herramientas y repositorios especializados. Cada uno cumple un rol crucial en el ciclo de vida de nuestros datos, desde la recolecci√≥n y procesamiento hasta la visualizaci√≥n final. A continuaci√≥n, un resumen de estas herramientas:\n\nqa: Este repositorio (https://github.com/argendatafundar/qa) estructura y simplifica los procesos de creaci√≥n y ejecuci√≥n de controles de calidad sobre los conjuntos de datos. Facilita la administraci√≥n del sistema de archivos y la interacci√≥n para el Control de Calidad (QA) entre investigadores y el equipo de datos.\ngeonomencladores: Contiene un diccionario de entidades geogr√°ficas normalizado (https://github.com/argendatafundar/geonomencladores) esencial para la consistencia en el uso de datos geogr√°ficos.\netl: Este proyecto (https://github.com/argendatafundar/etl) se centra en la armonizaci√≥n del proceso de Exploraci√≥n, Transformaci√≥n y Carga (ETL) de datos. Su objetivo es reducir la fricci√≥n en la actualizaci√≥n, automatizando pasos como la descarga de fuentes, la limpieza y la generaci√≥n de datasets para visualizaciones.\nargendataR: Un paquete de R (https://github.com/argendatafundar/argendataR) que ofrece funciones auxiliares para el proceso ETL, optimizando el flujo de trabajo con fuentes y resultados.\ndata: Aqu√≠ se comparten los datasets definitivos (https://github.com/argendatafundar/data), agrupados por t√≥picos y disponibles para descarga directa desde el sitio web de Argendata.\ndata-transformers: Una biblioteca para Python (https://github.com/argendatafundar/data-transformers) dise√±ada para facilitar la escritura, ejecuci√≥n, reproducibilidad y versionado del c√≥digo fuente que manipula datos estructurados. Su rol principal es formatear los datos para las visualizaciones del Frontend.\ntransformers: Este repositorio (https://github.com/argendatafundar/transformers) aloja los scripts de Python que act√∫an como ‚Äúrecetas‚Äù para transformar los datos de data al formato requerido por el Frontend para la visualizaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\nEl repositorio qa (https://github.com/argendatafundar/qa) es nuestro caballito de batalla para trabajar por la calidad y consistencia de los datos. Tiene dos funciones principales que resultan clave:\n\nOrganizaci√≥n y Administraci√≥n: qa establece una estructura clara y estandarizada para el sistema de archivos compartido. Esto es crucial para que todos los investigadores y el equipo de datos trabajen con la misma l√≥gica al guardar y buscar la informaci√≥n, minimizando as√≠ errores y datos duplicados. Asegura que los datos se almacenen de manera predecible, facilitando su gesti√≥n a largo plazo.\nControl de Calidad Interactivo: Facilita una colaboraci√≥n constante y efectiva entre los investigadores, que son quienes generan los datos, y nuestro equipo especializado. A trav√©s de este programa, se realizan validaciones sistem√°ticas que permiten detectar inconsistencias, anomal√≠as o errores en etapas tempranas. Por ejemplo, qa puede verificar que un campo num√©rico no contenga texto, que los rangos de fechas sean l√≥gicos, o que los valores de una variable categ√≥rica se ajusten a un listado predefinido. Al automatizar estos controles, qa acelera el ciclo de desarrollo, reduce la carga de trabajo manual y garantiza que solo los datos m√°s confiables y limpios avancen hacia las siguientes etapas de armonizaci√≥n y visualizaci√≥n. Es, en esencia, nuestro guardi√°n de la integridad de los datos.\n\n\n\n\nEl geonomenclador es un componente esencial en Argendata. Este funciona como un diccionario estandarizado de entidades geogr√°ficas (pa√≠ses y regiones). Su prop√≥sito principal es armonizar datos provenientes de distintas fuentes, procurando asegurar que todos se refieran a las mismas ubicaciones de manera consistente.\n\nPara construir esta herramienta vital, realizamos un exhaustivo trabajo de normalizaci√≥n de la nomenclatura de pa√≠ses. Esto implic√≥ comparar los c√≥digos estandarizados de diversas bases de datos. La priorizaci√≥n de estos c√≥digos se bas√≥ en la proporci√≥n de uso de cada base sobre los datasets de nuestro proyecto, garantizando que el nomenclador refleje las realidades de nuestros datos.\nPosteriormente, aplicamos t√©cnicas de fuzzy matching para unificar los nombres de distintos pa√≠ses y regiones. Este proceso fue importante para resolver variaciones en la escritura o denominaci√≥n de una misma entidad geogr√°fica, como se ilustra en el siguiente gr√°fico, donde cada flecha representa la similitud entre dos regiones basada en sus nombres.\n\n\n\n\nEl repositorio etl (https://github.com/argendatafundar/etl) es el motor que impulsa la generaci√≥n y transformaci√≥n de nuestros datos de forma program√°tica y reproducible. Contiene los scripts en R que garantizan un proceso sistem√°tico de Extracci√≥n, Transformaci√≥n y Carga (ETL).\nEste repositorio asegura la armonizaci√≥n del flujo de trabajo, lo que reduce la fricci√≥n en cada actualizaci√≥n de datos. Desde la descarga de fuentes raw hasta la limpieza y preparaci√≥n de los datasets finales para las visualizaciones, etl automatiza pasos cruciales para mantener la coherencia y la eficiencia.\nUn aspecto clave de etl es su enfoque en la reproducibilidad. Todos los scripts est√°n dise√±ados para ser modulares y parametrizables, facilitando la colaboraci√≥n entre investigadores y la actualizaci√≥n consistente de la informaci√≥n. Por ejemplo, promovemos el uso de c√≥digo que se adapte autom√°ticamente a cambios en los nombres de columnas a lo largo del tiempo, asegurando que los scripts sigan funcionando con nuevas versiones de las fuentes.\nLos resultados finales de estos procesos de etl se almacenan directamente en el repositorio data/, listos para alimentar las visualizaciones en Argendata.\n\n\n\nEl paquete argendataR (https://github.com/argendatafundar/argendataR) es una colecci√≥n de funciones en R dise√±ada para simplificar y estandarizar el flujo de trabajo con las fuentes de datos y los outputs generados en el proceso de armonizaci√≥n (ETL). Este paquete es una herramienta indispensable para investigadores y el equipo de datos, ya que agiliza tareas repetitivas y asegura la consistencia en el manejo de la informaci√≥n.\nPor ejemplo, una de sus funcionalidades centrales es la gesti√≥n de fuentes de datos, dividida en funciones para fuentes raw (originales) y clean (procesadas). Permite registrar y actualizar metadatos de las fuentes en Google Sheets, as√≠ como cargar y descargar archivos de Google Drive. Por ejemplo, con agregar_fuente_raw() y actualizar_fuente_raw(), los investigadores pueden gestionar las versiones de los datos brutos, mientras que agregar_fuente_clean() y actualizar_fuente_clean() se encargan de las versiones ya limpias y preparadas, asegurando que cumplan con los est√°ndares de Argendata (ej., nombres de columnas consistentes, formato long).\nR\n# Ejemplo de c√≥mo se registra una nueva fuente raw por primera vez \n\nlibrary(argendataR)\n## basic example code\nagregar_fuente_raw(url = \"https://www.ejemplo.com\",\n                   nombre = \"Ejemplo\",\n                   institucion = \"Ejemplo\",\n                   actualizable = TRUE,\n                   fecha_descarga = \"2021-01-01\",\n                   fecha_actualizar = \"2022-01-01\",\n                   path_raw = \"ejemplo.csv\",\n                   script = \"ejemplo.R\")\nAdem√°s, argendataR es clave en la generaci√≥n de los outputs finales. Su funci√≥n principal, write_output(), es crucial para generar los archivos .json con datos y metadatos que Argendata necesita para sus visualizaciones. Opcionalmente, tambi√©n puede exportar un .csv siguiendo los est√°ndares de Fundar (UTF-8, delimitadores espec√≠ficos), lo que garantiza la compatibilidad y facilidad de uso de nuestros datasets. Esta funci√≥n encapsula toda la complejidad del formato de salida, permitiendo a los investigadores centrarse en el procesamiento de los datos.\nR\n# Ejemplo simplificado de uso de write_output para un dataset # df_output ser√≠a el data frame ya procesado y listo \n\nwrite_output( data = df_output,\n                extension = 'csv',\n                output_name = 'A1_inb_pib',\n                subtopico = 'ACECON',\n                fuentes = 'World Development Indicators',\n                analista = 'Andr√©s Salles',\n                aclaraciones = NULL,\n                exportar = TRUE,\n                pk = c(\"anio\", \"iso3\"),\n                es_serie_tiempo = TRUE,\n                columna_indice_tiempo = \"anio\",\n                columna_geo_referencia = \"iso3\",\n                nivel_agregacion = \"pais\",\n                nullables = FALSE,\n                etiquetas_indicadores = list(\"diferencia_inb_pbi\" = \"Diferencia entre Ingreso Bruto Nacional y PBI\"),\n                unidades = \"Porcentaje respecto al PBI\",\n                classes = NULL)\nEn resumen, argendataR es la caja de herramientas para el flujo de trabajo de datos en Argendata, promoviendo la sistematizaci√≥n, la calidad y la eficiencia en la producci√≥n de conocimiento. Las salidas finales de estos procesos son las que se comparten y actualizan en el repositorio data y alimentan el sitio argendata.fund.ar para la descarga de datos.\n\n\n\nEl repositorio data (https://github.com/argendatafundar/data) es el punto central de almacenamiento y distribuci√≥n de los datasets finales que se publican en Argendata. Estos conjuntos de datos son el resultado del proceso semi-automatizado de reproducibilidad (ETL), lo que asegura su consistencia y calidad. B√°sicamente, data es la vidriera donde se consolidan y comparten los datos producidos por los investigadores, listos para su uso y descarga.\n\n\nPara mantener la claridad y facilitar el acceso, los datos en este repositorio est√°n meticulosamente organizados:\n\nCodificaci√≥n por T√≥pico: Cada t√≥pico de Argendata tiene un c√≥digo √∫nico de seis letras en may√∫sculas (ej., CAMCLI para ‚ÄúCambio Clim√°tico‚Äù, PRECIO para ‚ÄúInflaci√≥n‚Äù). Estos c√≥digos sirven como identificadores √∫nicos y estructuran el sistema de archivos del repositorio, agrupando los datasets por su tema principal.\nArchivos por Dataset: Dentro de la carpeta de cada t√≥pico, se encuentran dos tipos de archivos para cada dataset individual:\n\nEl dataset original limpio en formato .csv. Estos son los datos ya procesados y listos para ser utilizados.\nLos metadatos en formato .json, que acompa√±an a cada dataset con el mismo nombre. Estos archivos .json son cruciales porque proveen informaci√≥n detallada sobre el dataset, incluyendo sus fuentes (fuentes), tipo de extensi√≥n (extension), analistas responsables (analista), los tipos de datos por variable y otras especificaciones necesarias para su correcta interpretaci√≥n y uso.\n\nPor ejemplo, el dataset de inflaci√≥n PRECIO/12_tasa_de_inflacion_mensual_argentina_ene1989_dic1993 viene acompa√±ado de su .csv con las columnas fecha e inflacion_mensual, y un .json con sus metadatos (subtopico, output_name, extension, analista, fuentes). Este esquema garantiza que los usuarios no solo obtengan los datos, sino tambi√©n el contexto y la informaci√≥n necesaria para entenderlos y utilizarlos correctamente.\n\n\n\n\nLa organizaci√≥n del repositorio sigue un esquema l√≥gico que refleja la clasificaci√≥n por t√≥pico:\n‚îú‚îÄ‚îÄ TOPICO\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ CAMCLI\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.csv\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.json\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ COMEXT\n    ‚îú‚îÄ‚îÄ cambio_destinos_exportacion.csv\n    ‚îú‚îÄ‚îÄ cambio_destinos_exportacion.json\n    ...\nEsta estructura jer√°rquica facilita la navegaci√≥n y la identificaci√≥n de los conjuntos de datos, permitiendo a los usuarios encontrar r√°pidamente la informaci√≥n de su inter√©s.\nLos datos almacenados en este repositorio son los que se disponibilizan directamente para la descarga de los usuarios desde el sitio web de Argendata. Adem√°s, son la base para el procesamiento posterior realizado por el repositorio transformers, que los adapta para la visualizaci√≥n interactiva en el Frontend de argendata.fund.ar.\n\n\n\n\nEl repositorio data-transformers (https://github.com/argendatafundar/data-transformers) es una biblioteca de Python dise√±ada para ser un componente clave en el an√°lisis y procesamiento manual-asistido de datos estructurados dentro de Argendata. Su prop√≥sito principal es facilitar la escritura, ejecuci√≥n, reproducibilidad y el versionado del c√≥digo fuente que manipula estos datos.\nEsta herramienta se enfoca en el formato y la preparaci√≥n de los datos seg√∫n las necesidades espec√≠ficas del c√≥digo utilizado por el Frontend para las visualizaciones. No se trata de la generaci√≥n inicial de datos (eso ocurre en etl), sino de las transformaciones finales que optimizan los datasets para su presentaci√≥n interactiva. Esto puede incluir tareas como:\n\nFiltrado de valores: Seleccionar subconjuntos de datos relevantes para una visualizaci√≥n particular.\nSelecci√≥n y renombramiento de variables: Ajustar los nombres de las columnas o seleccionar solo aquellas que son necesarias para el gr√°fico final.\nAgregaciones o pivotes espec√≠ficos: Reestructurar los datos para que se adapten a los requisitos del componente de visualizaci√≥n.\n\nAl centralizar estas transformaciones en data-transformers, se garantiza que el paso final de preparaci√≥n de datos sea consistente y que cualquier cambio necesario en el formato de visualizaci√≥n se aplique de manera eficiente y reproducible. Esto es vital para mantener la agilidad en el desarrollo de nuevas visualizaciones y la calidad de la informaci√≥n mostrada en el sitio.\n\n\n\nEl repositorio transformers (https://github.com/argendatafundar/transformers) contiene los scripts en Python que act√∫an como ‚Äúrecetas‚Äù ejecutables y autocontenidas. B√°sicamente, son programas dise√±ados para aplicar una serie de instrucciones de mutaci√≥n a un dataset, transform√°ndolo espec√≠ficamente para su visualizaci√≥n en el sitio web de Argendata.\nEstos scripts van un paso m√°s all√° de la armonizaci√≥n de datos que se realiza en el proceso etl. Su objetivo es una normalizaci√≥n m√°s estricta, donde se deja √∫nicamente la informaci√≥n imprescindible para que el gr√°fico se pueda visualizar. As√≠, se generan archivos mucho m√°s livianos y optimizados para el Frontend.\nCada transformer se explica por s√≠ mismo en su funcionalidad: describe lo que hace, lo que facilita mucho su comprensi√≥n y garantiza su f√°cil ejecuci√≥n en un proceso automatizado. La relaci√≥n con data-transformers es directa: data-transformers es la librer√≠a que provee las funciones base que estos scripts utilizan para construir sus ‚Äúrecetas‚Äù.\nEl trabajo en este repositorio se organiza por t√≥picos, reflejando la estructura de Argendata:\n‚îú‚îÄ‚îÄ TOPICO\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ AGROPE\n‚îÇ   ‚îú‚îÄ‚îÄ mappings.json\n‚îÇ   ‚îú‚îÄ‚îÄ TOPICO_gXX_transformer.py\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ TRANEN\n    ‚îú‚îÄ‚îÄ mappings.json\n    ‚îú‚îÄ‚îÄ TRANEN_g01_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g02_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g03_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g04_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g05_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g06_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g07_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g08_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g09_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g10_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g11_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g12_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g13_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g14_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g15_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g16_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g17_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g18_transformer.py\n    ‚îî‚îÄ‚îÄ TRANEN_g19_transformer.py\n\nCada dataset publicado en data (resultado del proceso ETL) tiene una correspondencia con uno o m√°s transformers aqu√≠. Un dataset de data (por ejemplo, matriz_prim_mundo_historic_larga.csv en data) es el ‚Äúinsumo‚Äù para un transformer (como TRANEN_g01_transformer.py en transformers. Esta equivalencia est√° documentada en un archivo mappings.json dentro de cada subdirectorio de t√≥pico en este repositorio.\nUn ejemplo de c√≥mo opera un transformer es el TRANEN_g01_transformer.py, que toma un DataFrame y aplica una serie de transformaciones usando funciones de data-transformers (como query, rename_cols y sort_values) para preparar los datos para la visualizaci√≥n final. Esto asegura que los datos sean presentados de forma √≥ptima y coherente en la web.\n\nPython\n# Extracto del c√≥digo de TRANEN_g01_transformer.py \n\n\nfrom pandas import DataFrame\nfrom data_transformers import chain, transformer\n\n\n#  DEFINITIONS_START\n@transformer.convert\ndef query(df: DataFrame, condition: str):\n    df = df.query(condition)    \n    return df\n\n@transformer.convert\ndef rename_cols(df: DataFrame, map):\n    df = df.rename(columns=map)\n    return df\n\n@transformer.convert\ndef sort_values(df: DataFrame, how: str, by: list):\n    if how not in ['ascending', 'descending']:\n        raise ValueError('how must be either \"ascending\" or \"descending\"')\n    return df.sort_values(by=by, ascending=how == 'ascending')\n#  DEFINITIONS_END\n\n\n#  PIPELINE_START\npipeline = chain(\nquery(condition='tipo_energia != \"Total\"'),\n    rename_cols(map={'tipo_energia': 'indicador', 'valor_en_twh': 'valor'}),\n    sort_values(how='ascending', by=['anio', 'indicador'])\n)\n#  PIPELINE_END\n\n\n#  start()\n#  RangeIndex: 836 entries, 0 to 835\n#  Data columns (total 4 columns):\n#   #   Column        Non-Null Count  Dtype  \n#  ---  ------        --------------  -----  \n#   0   anio          836 non-null    int64  \n#   1   tipo_energia  836 non-null    object \n#   2   valor_en_twh  836 non-null    float64\n#   3   porcentaje    836 non-null    float64\n#  \n#  |    |   anio | tipo_energia     |   valor_en_twh |   porcentaje |\n#  |---:|-------:|:-----------------|---------------:|-------------:|\n#  |  0 |   1800 | Otras renovables |              0 |            0 |\n#  \n#  ------------------------------\n#  \n#  query(condition='tipo_energia != \"Total\"')\n#  Index: 760 entries, 0 to 759\n#  Data columns (total 4 columns):\n#   #   Column        Non-Null Count  Dtype  \n#  ---  ------        --------------  -----  \n#   0   anio          760 non-null    int64  \n#   1   tipo_energia  760 non-null    object \n#   2   valor_en_twh  760 non-null    float64\n#   3   porcentaje    760 non-null    float64\n#  \n#  |    |   anio | tipo_energia     |   valor_en_twh |   porcentaje |\n#  |---:|-------:|:-----------------|---------------:|-------------:|\n#  |  0 |   1800 | Otras renovables |              0 |            0 |\n#  \n#  ------------------------------\n#  \n#  rename_cols(map={'tipo_energia': 'indicador', 'valor_en_twh': 'valor'})\n#  Index: 760 entries, 0 to 759\n#  Data columns (total 4 columns):\n#   #   Column      Non-Null Count  Dtype  \n#  ---  ------      --------------  -----  \n#   0   anio        760 non-null    int64  \n#   1   indicador   760 non-null    object \n#   2   valor       760 non-null    float64\n#   3   porcentaje  760 non-null    float64\n#  \n#  |    |   anio | indicador        |   valor |   porcentaje |\n#  |---:|-------:|:-----------------|--------:|-------------:|\n#  |  0 |   1800 | Otras renovables |       0 |            0 |\n#  \n#  ------------------------------\n#  \n#  sort_values(how='ascending', by=['anio', 'indicador'])\n#  Index: 760 entries, 76 to 227\n#  Data columns (total 4 columns):\n#   #   Column      Non-Null Count  Dtype  \n#  ---  ------      --------------  -----  \n#   0   anio        760 non-null    int64  \n#   1   indicador   760 non-null    object \n#   2   valor       760 non-null    float64\n#   3   porcentaje  760 non-null    float64\n#  \n#  |    |   anio | indicador       |   valor |   porcentaje |\n#  |---:|-------:|:----------------|--------:|-------------:|\n#  | 76 |   1800 | Biocombustibles |       0 |            0 |\n#  \n#  ------------------------------\n#  \n\nEste proceso en dos etapas (primero ETL, luego transformaci√≥n para Frontend) asegura la calidad del dato base y al mismo tiempo la flexibilidad y eficiencia en la presentaci√≥n visual de la informaci√≥n.\n\n‚Äî‚Äî‚Äì\n\n\n\nHasta ac√° un resumen de lo que fue el primer a√±o, centrado en el camino de los datos. En pr√≥ximas publicaciones, espero con blogueros inivtados, avanzaremos con m√°s detalle sobre algunos de los procesos, con especial atenci√≥n al trabajo dedicado a la visualizaci√≥n.\n\nHasta la pr√≥xima y feliz cumple a nosotros =)"
  },
  {
    "objectID": "posts/2025-06-04_intro_argendata.html#dise√±ar-para-durar-los-fundamentos-t√©cnicos-de-argendata",
    "href": "posts/2025-06-04_intro_argendata.html#dise√±ar-para-durar-los-fundamentos-t√©cnicos-de-argendata",
    "title": "argendata: El camino de los datos",
    "section": "",
    "text": "Argendata es una iniciativa de fundar que busca organizar, armonizar y publicar datos sobre Argentina con una l√≥gica clara: que sirvan para entender mejor, discutir mejor y decidir mejor. En definitiva, un peque√±o aporte que tiene una enorme ambici√≥n: transformar la Argentina.\nPero no se trata solo de subir textos y gr√°ficos. Detr√°s del sitio hay una estructura t√©cnica que permite mantener los datos vivos, trazables y disponibles. Una arquitectura pensada para que el contenido sea m√°s que informaci√≥n: que sea evidencia\n\n\nTL;DR\n\nEn Argendata, la calidad, transparencia y reproducibilidad de los datos son fundamentales para nutrir el debate p√∫blico con informaci√≥n confiable. Nuestros procesos est√°n dise√±ados para procurar que cada dato sea robusto y accesible. Nos basamos en tres pilares esenciales: la administraci√≥n de recursos, que asegura la coherencia y armonizaci√≥n de los datos; un riguroso control de calidad y reportes; y la armonizaci√≥n del c√≥digo fuente, clave para la reproducibilidad y actualizaci√≥n constante. Este enfoque nos permite transformar los datos generados por investigadores en las visualizaciones claras y √∫tiles que encontr√°s en nuestro sitio.\n\n\n\n\nPara lograr todo esto, en Argendata utilizamos un conjunto de herramientas y repositorios especializados. Cada uno cumple un rol crucial en el ciclo de vida de nuestros datos, desde la recolecci√≥n y procesamiento hasta la visualizaci√≥n final. A continuaci√≥n, un resumen de estas herramientas:\n\nqa: Este repositorio (https://github.com/argendatafundar/qa) estructura y simplifica los procesos de creaci√≥n y ejecuci√≥n de controles de calidad sobre los conjuntos de datos. Facilita la administraci√≥n del sistema de archivos y la interacci√≥n para el Control de Calidad (QA) entre investigadores y el equipo de datos.\ngeonomencladores: Contiene un diccionario de entidades geogr√°ficas normalizado (https://github.com/argendatafundar/geonomencladores) esencial para la consistencia en el uso de datos geogr√°ficos.\netl: Este proyecto (https://github.com/argendatafundar/etl) se centra en la armonizaci√≥n del proceso de Exploraci√≥n, Transformaci√≥n y Carga (ETL) de datos. Su objetivo es reducir la fricci√≥n en la actualizaci√≥n, automatizando pasos como la descarga de fuentes, la limpieza y la generaci√≥n de datasets para visualizaciones.\nargendataR: Un paquete de R (https://github.com/argendatafundar/argendataR) que ofrece funciones auxiliares para el proceso ETL, optimizando el flujo de trabajo con fuentes y resultados.\ndata: Aqu√≠ se comparten los datasets definitivos (https://github.com/argendatafundar/data), agrupados por t√≥picos y disponibles para descarga directa desde el sitio web de Argendata.\ndata-transformers: Una biblioteca para Python (https://github.com/argendatafundar/data-transformers) dise√±ada para facilitar la escritura, ejecuci√≥n, reproducibilidad y versionado del c√≥digo fuente que manipula datos estructurados. Su rol principal es formatear los datos para las visualizaciones del Frontend.\ntransformers: Este repositorio (https://github.com/argendatafundar/transformers) aloja los scripts de Python que act√∫an como ‚Äúrecetas‚Äù para transformar los datos de data al formato requerido por el Frontend para la visualizaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\nEl repositorio qa (https://github.com/argendatafundar/qa) es nuestro caballito de batalla para trabajar por la calidad y consistencia de los datos. Tiene dos funciones principales que resultan clave:\n\nOrganizaci√≥n y Administraci√≥n: qa establece una estructura clara y estandarizada para el sistema de archivos compartido. Esto es crucial para que todos los investigadores y el equipo de datos trabajen con la misma l√≥gica al guardar y buscar la informaci√≥n, minimizando as√≠ errores y datos duplicados. Asegura que los datos se almacenen de manera predecible, facilitando su gesti√≥n a largo plazo.\nControl de Calidad Interactivo: Facilita una colaboraci√≥n constante y efectiva entre los investigadores, que son quienes generan los datos, y nuestro equipo especializado. A trav√©s de este programa, se realizan validaciones sistem√°ticas que permiten detectar inconsistencias, anomal√≠as o errores en etapas tempranas. Por ejemplo, qa puede verificar que un campo num√©rico no contenga texto, que los rangos de fechas sean l√≥gicos, o que los valores de una variable categ√≥rica se ajusten a un listado predefinido. Al automatizar estos controles, qa acelera el ciclo de desarrollo, reduce la carga de trabajo manual y garantiza que solo los datos m√°s confiables y limpios avancen hacia las siguientes etapas de armonizaci√≥n y visualizaci√≥n. Es, en esencia, nuestro guardi√°n de la integridad de los datos.\n\n\n\n\nEl geonomenclador es un componente esencial en Argendata. Este funciona como un diccionario estandarizado de entidades geogr√°ficas (pa√≠ses y regiones). Su prop√≥sito principal es armonizar datos provenientes de distintas fuentes, procurando asegurar que todos se refieran a las mismas ubicaciones de manera consistente.\n\nPara construir esta herramienta vital, realizamos un exhaustivo trabajo de normalizaci√≥n de la nomenclatura de pa√≠ses. Esto implic√≥ comparar los c√≥digos estandarizados de diversas bases de datos. La priorizaci√≥n de estos c√≥digos se bas√≥ en la proporci√≥n de uso de cada base sobre los datasets de nuestro proyecto, garantizando que el nomenclador refleje las realidades de nuestros datos.\nPosteriormente, aplicamos t√©cnicas de fuzzy matching para unificar los nombres de distintos pa√≠ses y regiones. Este proceso fue importante para resolver variaciones en la escritura o denominaci√≥n de una misma entidad geogr√°fica, como se ilustra en el siguiente gr√°fico, donde cada flecha representa la similitud entre dos regiones basada en sus nombres.\n\n\n\n\nEl repositorio etl (https://github.com/argendatafundar/etl) es el motor que impulsa la generaci√≥n y transformaci√≥n de nuestros datos de forma program√°tica y reproducible. Contiene los scripts en R que garantizan un proceso sistem√°tico de Extracci√≥n, Transformaci√≥n y Carga (ETL).\nEste repositorio asegura la armonizaci√≥n del flujo de trabajo, lo que reduce la fricci√≥n en cada actualizaci√≥n de datos. Desde la descarga de fuentes raw hasta la limpieza y preparaci√≥n de los datasets finales para las visualizaciones, etl automatiza pasos cruciales para mantener la coherencia y la eficiencia.\nUn aspecto clave de etl es su enfoque en la reproducibilidad. Todos los scripts est√°n dise√±ados para ser modulares y parametrizables, facilitando la colaboraci√≥n entre investigadores y la actualizaci√≥n consistente de la informaci√≥n. Por ejemplo, promovemos el uso de c√≥digo que se adapte autom√°ticamente a cambios en los nombres de columnas a lo largo del tiempo, asegurando que los scripts sigan funcionando con nuevas versiones de las fuentes.\nLos resultados finales de estos procesos de etl se almacenan directamente en el repositorio data/, listos para alimentar las visualizaciones en Argendata.\n\n\n\nEl paquete argendataR (https://github.com/argendatafundar/argendataR) es una colecci√≥n de funciones en R dise√±ada para simplificar y estandarizar el flujo de trabajo con las fuentes de datos y los outputs generados en el proceso de armonizaci√≥n (ETL). Este paquete es una herramienta indispensable para investigadores y el equipo de datos, ya que agiliza tareas repetitivas y asegura la consistencia en el manejo de la informaci√≥n.\nPor ejemplo, una de sus funcionalidades centrales es la gesti√≥n de fuentes de datos, dividida en funciones para fuentes raw (originales) y clean (procesadas). Permite registrar y actualizar metadatos de las fuentes en Google Sheets, as√≠ como cargar y descargar archivos de Google Drive. Por ejemplo, con agregar_fuente_raw() y actualizar_fuente_raw(), los investigadores pueden gestionar las versiones de los datos brutos, mientras que agregar_fuente_clean() y actualizar_fuente_clean() se encargan de las versiones ya limpias y preparadas, asegurando que cumplan con los est√°ndares de Argendata (ej., nombres de columnas consistentes, formato long).\nR\n# Ejemplo de c√≥mo se registra una nueva fuente raw por primera vez \n\nlibrary(argendataR)\n## basic example code\nagregar_fuente_raw(url = \"https://www.ejemplo.com\",\n                   nombre = \"Ejemplo\",\n                   institucion = \"Ejemplo\",\n                   actualizable = TRUE,\n                   fecha_descarga = \"2021-01-01\",\n                   fecha_actualizar = \"2022-01-01\",\n                   path_raw = \"ejemplo.csv\",\n                   script = \"ejemplo.R\")\nAdem√°s, argendataR es clave en la generaci√≥n de los outputs finales. Su funci√≥n principal, write_output(), es crucial para generar los archivos .json con datos y metadatos que Argendata necesita para sus visualizaciones. Opcionalmente, tambi√©n puede exportar un .csv siguiendo los est√°ndares de Fundar (UTF-8, delimitadores espec√≠ficos), lo que garantiza la compatibilidad y facilidad de uso de nuestros datasets. Esta funci√≥n encapsula toda la complejidad del formato de salida, permitiendo a los investigadores centrarse en el procesamiento de los datos.\nR\n# Ejemplo simplificado de uso de write_output para un dataset # df_output ser√≠a el data frame ya procesado y listo \n\nwrite_output( data = df_output,\n                extension = 'csv',\n                output_name = 'A1_inb_pib',\n                subtopico = 'ACECON',\n                fuentes = 'World Development Indicators',\n                analista = 'Andr√©s Salles',\n                aclaraciones = NULL,\n                exportar = TRUE,\n                pk = c(\"anio\", \"iso3\"),\n                es_serie_tiempo = TRUE,\n                columna_indice_tiempo = \"anio\",\n                columna_geo_referencia = \"iso3\",\n                nivel_agregacion = \"pais\",\n                nullables = FALSE,\n                etiquetas_indicadores = list(\"diferencia_inb_pbi\" = \"Diferencia entre Ingreso Bruto Nacional y PBI\"),\n                unidades = \"Porcentaje respecto al PBI\",\n                classes = NULL)\nEn resumen, argendataR es la caja de herramientas para el flujo de trabajo de datos en Argendata, promoviendo la sistematizaci√≥n, la calidad y la eficiencia en la producci√≥n de conocimiento. Las salidas finales de estos procesos son las que se comparten y actualizan en el repositorio data y alimentan el sitio argendata.fund.ar para la descarga de datos.\n\n\n\nEl repositorio data (https://github.com/argendatafundar/data) es el punto central de almacenamiento y distribuci√≥n de los datasets finales que se publican en Argendata. Estos conjuntos de datos son el resultado del proceso semi-automatizado de reproducibilidad (ETL), lo que asegura su consistencia y calidad. B√°sicamente, data es la vidriera donde se consolidan y comparten los datos producidos por los investigadores, listos para su uso y descarga.\n\n\nPara mantener la claridad y facilitar el acceso, los datos en este repositorio est√°n meticulosamente organizados:\n\nCodificaci√≥n por T√≥pico: Cada t√≥pico de Argendata tiene un c√≥digo √∫nico de seis letras en may√∫sculas (ej., CAMCLI para ‚ÄúCambio Clim√°tico‚Äù, PRECIO para ‚ÄúInflaci√≥n‚Äù). Estos c√≥digos sirven como identificadores √∫nicos y estructuran el sistema de archivos del repositorio, agrupando los datasets por su tema principal.\nArchivos por Dataset: Dentro de la carpeta de cada t√≥pico, se encuentran dos tipos de archivos para cada dataset individual:\n\nEl dataset original limpio en formato .csv. Estos son los datos ya procesados y listos para ser utilizados.\nLos metadatos en formato .json, que acompa√±an a cada dataset con el mismo nombre. Estos archivos .json son cruciales porque proveen informaci√≥n detallada sobre el dataset, incluyendo sus fuentes (fuentes), tipo de extensi√≥n (extension), analistas responsables (analista), los tipos de datos por variable y otras especificaciones necesarias para su correcta interpretaci√≥n y uso.\n\nPor ejemplo, el dataset de inflaci√≥n PRECIO/12_tasa_de_inflacion_mensual_argentina_ene1989_dic1993 viene acompa√±ado de su .csv con las columnas fecha e inflacion_mensual, y un .json con sus metadatos (subtopico, output_name, extension, analista, fuentes). Este esquema garantiza que los usuarios no solo obtengan los datos, sino tambi√©n el contexto y la informaci√≥n necesaria para entenderlos y utilizarlos correctamente.\n\n\n\n\nLa organizaci√≥n del repositorio sigue un esquema l√≥gico que refleja la clasificaci√≥n por t√≥pico:\n‚îú‚îÄ‚îÄ TOPICO\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ CAMCLI\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.csv\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.json\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ COMEXT\n    ‚îú‚îÄ‚îÄ cambio_destinos_exportacion.csv\n    ‚îú‚îÄ‚îÄ cambio_destinos_exportacion.json\n    ...\nEsta estructura jer√°rquica facilita la navegaci√≥n y la identificaci√≥n de los conjuntos de datos, permitiendo a los usuarios encontrar r√°pidamente la informaci√≥n de su inter√©s.\nLos datos almacenados en este repositorio son los que se disponibilizan directamente para la descarga de los usuarios desde el sitio web de Argendata. Adem√°s, son la base para el procesamiento posterior realizado por el repositorio transformers, que los adapta para la visualizaci√≥n interactiva en el Frontend de argendata.fund.ar.\n\n\n\n\nEl repositorio data-transformers (https://github.com/argendatafundar/data-transformers) es una biblioteca de Python dise√±ada para ser un componente clave en el an√°lisis y procesamiento manual-asistido de datos estructurados dentro de Argendata. Su prop√≥sito principal es facilitar la escritura, ejecuci√≥n, reproducibilidad y el versionado del c√≥digo fuente que manipula estos datos.\nEsta herramienta se enfoca en el formato y la preparaci√≥n de los datos seg√∫n las necesidades espec√≠ficas del c√≥digo utilizado por el Frontend para las visualizaciones. No se trata de la generaci√≥n inicial de datos (eso ocurre en etl), sino de las transformaciones finales que optimizan los datasets para su presentaci√≥n interactiva. Esto puede incluir tareas como:\n\nFiltrado de valores: Seleccionar subconjuntos de datos relevantes para una visualizaci√≥n particular.\nSelecci√≥n y renombramiento de variables: Ajustar los nombres de las columnas o seleccionar solo aquellas que son necesarias para el gr√°fico final.\nAgregaciones o pivotes espec√≠ficos: Reestructurar los datos para que se adapten a los requisitos del componente de visualizaci√≥n.\n\nAl centralizar estas transformaciones en data-transformers, se garantiza que el paso final de preparaci√≥n de datos sea consistente y que cualquier cambio necesario en el formato de visualizaci√≥n se aplique de manera eficiente y reproducible. Esto es vital para mantener la agilidad en el desarrollo de nuevas visualizaciones y la calidad de la informaci√≥n mostrada en el sitio.\n\n\n\nEl repositorio transformers (https://github.com/argendatafundar/transformers) contiene los scripts en Python que act√∫an como ‚Äúrecetas‚Äù ejecutables y autocontenidas. B√°sicamente, son programas dise√±ados para aplicar una serie de instrucciones de mutaci√≥n a un dataset, transform√°ndolo espec√≠ficamente para su visualizaci√≥n en el sitio web de Argendata.\nEstos scripts van un paso m√°s all√° de la armonizaci√≥n de datos que se realiza en el proceso etl. Su objetivo es una normalizaci√≥n m√°s estricta, donde se deja √∫nicamente la informaci√≥n imprescindible para que el gr√°fico se pueda visualizar. As√≠, se generan archivos mucho m√°s livianos y optimizados para el Frontend.\nCada transformer se explica por s√≠ mismo en su funcionalidad: describe lo que hace, lo que facilita mucho su comprensi√≥n y garantiza su f√°cil ejecuci√≥n en un proceso automatizado. La relaci√≥n con data-transformers es directa: data-transformers es la librer√≠a que provee las funciones base que estos scripts utilizan para construir sus ‚Äúrecetas‚Äù.\nEl trabajo en este repositorio se organiza por t√≥picos, reflejando la estructura de Argendata:\n‚îú‚îÄ‚îÄ TOPICO\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ AGROPE\n‚îÇ   ‚îú‚îÄ‚îÄ mappings.json\n‚îÇ   ‚îú‚îÄ‚îÄ TOPICO_gXX_transformer.py\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ TRANEN\n    ‚îú‚îÄ‚îÄ mappings.json\n    ‚îú‚îÄ‚îÄ TRANEN_g01_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g02_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g03_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g04_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g05_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g06_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g07_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g08_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g09_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g10_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g11_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g12_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g13_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g14_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g15_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g16_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g17_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g18_transformer.py\n    ‚îî‚îÄ‚îÄ TRANEN_g19_transformer.py\n\nCada dataset publicado en data (resultado del proceso ETL) tiene una correspondencia con uno o m√°s transformers aqu√≠. Un dataset de data (por ejemplo, matriz_prim_mundo_historic_larga.csv en data) es el ‚Äúinsumo‚Äù para un transformer (como TRANEN_g01_transformer.py en transformers. Esta equivalencia est√° documentada en un archivo mappings.json dentro de cada subdirectorio de t√≥pico en este repositorio.\nUn ejemplo de c√≥mo opera un transformer es el TRANEN_g01_transformer.py, que toma un DataFrame y aplica una serie de transformaciones usando funciones de data-transformers (como query, rename_cols y sort_values) para preparar los datos para la visualizaci√≥n final. Esto asegura que los datos sean presentados de forma √≥ptima y coherente en la web.\n\nPython\n# Extracto del c√≥digo de TRANEN_g01_transformer.py \n\n\nfrom pandas import DataFrame\nfrom data_transformers import chain, transformer\n\n\n#  DEFINITIONS_START\n@transformer.convert\ndef query(df: DataFrame, condition: str):\n    df = df.query(condition)    \n    return df\n\n@transformer.convert\ndef rename_cols(df: DataFrame, map):\n    df = df.rename(columns=map)\n    return df\n\n@transformer.convert\ndef sort_values(df: DataFrame, how: str, by: list):\n    if how not in ['ascending', 'descending']:\n        raise ValueError('how must be either \"ascending\" or \"descending\"')\n    return df.sort_values(by=by, ascending=how == 'ascending')\n#  DEFINITIONS_END\n\n\n#  PIPELINE_START\npipeline = chain(\nquery(condition='tipo_energia != \"Total\"'),\n    rename_cols(map={'tipo_energia': 'indicador', 'valor_en_twh': 'valor'}),\n    sort_values(how='ascending', by=['anio', 'indicador'])\n)\n#  PIPELINE_END\n\n\n#  start()\n#  RangeIndex: 836 entries, 0 to 835\n#  Data columns (total 4 columns):\n#   #   Column        Non-Null Count  Dtype  \n#  ---  ------        --------------  -----  \n#   0   anio          836 non-null    int64  \n#   1   tipo_energia  836 non-null    object \n#   2   valor_en_twh  836 non-null    float64\n#   3   porcentaje    836 non-null    float64\n#  \n#  |    |   anio | tipo_energia     |   valor_en_twh |   porcentaje |\n#  |---:|-------:|:-----------------|---------------:|-------------:|\n#  |  0 |   1800 | Otras renovables |              0 |            0 |\n#  \n#  ------------------------------\n#  \n#  query(condition='tipo_energia != \"Total\"')\n#  Index: 760 entries, 0 to 759\n#  Data columns (total 4 columns):\n#   #   Column        Non-Null Count  Dtype  \n#  ---  ------        --------------  -----  \n#   0   anio          760 non-null    int64  \n#   1   tipo_energia  760 non-null    object \n#   2   valor_en_twh  760 non-null    float64\n#   3   porcentaje    760 non-null    float64\n#  \n#  |    |   anio | tipo_energia     |   valor_en_twh |   porcentaje |\n#  |---:|-------:|:-----------------|---------------:|-------------:|\n#  |  0 |   1800 | Otras renovables |              0 |            0 |\n#  \n#  ------------------------------\n#  \n#  rename_cols(map={'tipo_energia': 'indicador', 'valor_en_twh': 'valor'})\n#  Index: 760 entries, 0 to 759\n#  Data columns (total 4 columns):\n#   #   Column      Non-Null Count  Dtype  \n#  ---  ------      --------------  -----  \n#   0   anio        760 non-null    int64  \n#   1   indicador   760 non-null    object \n#   2   valor       760 non-null    float64\n#   3   porcentaje  760 non-null    float64\n#  \n#  |    |   anio | indicador        |   valor |   porcentaje |\n#  |---:|-------:|:-----------------|--------:|-------------:|\n#  |  0 |   1800 | Otras renovables |       0 |            0 |\n#  \n#  ------------------------------\n#  \n#  sort_values(how='ascending', by=['anio', 'indicador'])\n#  Index: 760 entries, 76 to 227\n#  Data columns (total 4 columns):\n#   #   Column      Non-Null Count  Dtype  \n#  ---  ------      --------------  -----  \n#   0   anio        760 non-null    int64  \n#   1   indicador   760 non-null    object \n#   2   valor       760 non-null    float64\n#   3   porcentaje  760 non-null    float64\n#  \n#  |    |   anio | indicador       |   valor |   porcentaje |\n#  |---:|-------:|:----------------|--------:|-------------:|\n#  | 76 |   1800 | Biocombustibles |       0 |            0 |\n#  \n#  ------------------------------\n#  \n\nEste proceso en dos etapas (primero ETL, luego transformaci√≥n para Frontend) asegura la calidad del dato base y al mismo tiempo la flexibilidad y eficiencia en la presentaci√≥n visual de la informaci√≥n.\n\n‚Äî‚Äî‚Äì\n\n\n\nHasta ac√° un resumen de lo que fue el primer a√±o, centrado en el camino de los datos. En pr√≥ximas publicaciones, espero con blogueros inivtados, avanzaremos con m√°s detalle sobre algunos de los procesos, con especial atenci√≥n al trabajo dedicado a la visualizaci√≥n.\n\nHasta la pr√≥xima y feliz cumple a nosotros =)"
  },
  {
    "objectID": "posts/2025-06-04_intro_argendata.html#los-datos-el-coraz√≥n-de-argendata",
    "href": "posts/2025-06-04_intro_argendata.html#los-datos-el-coraz√≥n-de-argendata",
    "title": "argendata: El camino de los datos",
    "section": "",
    "text": "TL;DR\n\nEn Argendata, la calidad, transparencia y reproducibilidad de los datos son fundamentales para nutrir el debate p√∫blico con informaci√≥n confiable. Nuestros procesos est√°n dise√±ados para procurar que cada dato sea robusto y accesible. Nos basamos en tres pilares esenciales: la administraci√≥n de recursos, que asegura la coherencia y armonizaci√≥n de los datos; un riguroso control de calidad y reportes; y la armonizaci√≥n del c√≥digo fuente, clave para la reproducibilidad y actualizaci√≥n constante. Este enfoque nos permite transformar los datos generados por investigadores en las visualizaciones claras y √∫tiles que encontr√°s en nuestro sitio.\n\n\nPara lograr todo esto, en Argendata utilizamos un conjunto de herramientas y repositorios especializados. Cada uno cumple un rol crucial en el ciclo de vida de nuestros datos, desde la recolecci√≥n y procesamiento hasta la visualizaci√≥n final. A continuaci√≥n, un resumen de estas herramientas:\n\nqa: Este repositorio (https://github.com/argendatafundar/qa) estructura y simplifica los procesos de creaci√≥n y ejecuci√≥n de controles de calidad sobre los conjuntos de datos. Facilita la administraci√≥n del sistema de archivos y la interacci√≥n para el Control de Calidad (QA) entre investigadores y el equipo de datos.\ngeonomencladores: Contiene un diccionario de entidades geogr√°ficas normalizado (https://github.com/argendatafundar/geonomencladores) esencial para la consistencia en el uso de datos geogr√°ficos.\netl: Este proyecto (https://github.com/argendatafundar/etl) se centra en la armonizaci√≥n del proceso de Exploraci√≥n, Transformaci√≥n y Carga (ETL) de datos. Su objetivo es reducir la fricci√≥n en la actualizaci√≥n, automatizando pasos como la descarga de fuentes, la limpieza y la generaci√≥n de datasets para visualizaciones.\nargendataR: Un paquete de R (https://github.com/argendatafundar/argendataR) que ofrece funciones auxiliares para el proceso ETL, optimizando el flujo de trabajo con fuentes y resultados.\ndata: Aqu√≠ se comparten los datasets definitivos (https://github.com/argendatafundar/data), agrupados por t√≥picos y disponibles para descarga directa desde el sitio web de Argendata.\ndata-transformers: Una biblioteca para Python (https://github.com/argendatafundar/data-transformers) dise√±ada para facilitar la escritura, ejecuci√≥n, reproducibilidad y versionado del c√≥digo fuente que manipula datos estructurados. Su rol principal es formatear los datos para las visualizaciones del Frontend.\ntransformers: Este repositorio (https://github.com/argendatafundar/transformers) aloja los scripts de Python que act√∫an como ‚Äúrecetas‚Äù para transformar los datos de data al formato requerido por el Frontend para la visualizaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\nEl repositorio qa (https://github.com/argendatafundar/qa) es nuestro caballito de batalla para trabajar por la calidad y consistencia de los datos. Tiene dos funciones principales que resultan clave:\n\nOrganizaci√≥n y Administraci√≥n: qa establece una estructura clara y estandarizada para el sistema de archivos compartido. Esto es crucial para que todos los investigadores y el equipo de datos trabajen con la misma l√≥gica al guardar y buscar la informaci√≥n, minimizando as√≠ errores y datos duplicados. Asegura que los datos se almacenen de manera predecible, facilitando su gesti√≥n a largo plazo.\nControl de Calidad Interactivo: Facilita una colaboraci√≥n constante y efectiva entre los investigadores, que son quienes generan los datos, y nuestro equipo especializado. A trav√©s de este programa, se realizan validaciones sistem√°ticas que permiten detectar inconsistencias, anomal√≠as o errores en etapas tempranas. Por ejemplo, qa puede verificar que un campo num√©rico no contenga texto, que los rangos de fechas sean l√≥gicos, o que los valores de una variable categ√≥rica se ajusten a un listado predefinido. Al automatizar estos controles, qa acelera el ciclo de desarrollo, reduce la carga de trabajo manual y garantiza que solo los datos m√°s confiables y limpios avancen hacia las siguientes etapas de armonizaci√≥n y visualizaci√≥n. Es, en esencia, nuestro guardi√°n de la integridad de los datos.\n\n\n\n\nEl geonomenclador es un componente esencial en Argendata. Este funciona como un diccionario estandarizado de entidades geogr√°ficas (pa√≠ses y regiones). Su prop√≥sito principal es armonizar datos provenientes de distintas fuentes, procurando asegurar que todos se refieran a las mismas ubicaciones de manera consistente.\n\nPara construir esta herramienta vital, realizamos un exhaustivo trabajo de normalizaci√≥n de la nomenclatura de pa√≠ses. Esto implic√≥ comparar los c√≥digos estandarizados de diversas bases de datos. La priorizaci√≥n de estos c√≥digos se bas√≥ en la proporci√≥n de uso de cada base sobre los datasets de nuestro proyecto, garantizando que el nomenclador refleje las realidades de nuestros datos.\nPosteriormente, aplicamos t√©cnicas de fuzzy matching para unificar los nombres de distintos pa√≠ses y regiones. Este proceso fue importante para resolver variaciones en la escritura o denominaci√≥n de una misma entidad geogr√°fica, como se ilustra en el siguiente gr√°fico, donde cada flecha representa la similitud entre dos regiones basada en sus nombres.\n\n\n\n\nEl repositorio etl (https://github.com/argendatafundar/etl) es el motor que impulsa la generaci√≥n y transformaci√≥n de nuestros datos de forma program√°tica y reproducible. Contiene los scripts en R que garantizan un proceso sistem√°tico de Extracci√≥n, Transformaci√≥n y Carga (ETL).\nEste repositorio asegura la armonizaci√≥n del flujo de trabajo, lo que reduce la fricci√≥n en cada actualizaci√≥n de datos. Desde la descarga de fuentes raw hasta la limpieza y preparaci√≥n de los datasets finales para las visualizaciones, etl automatiza pasos cruciales para mantener la coherencia y la eficiencia.\nUn aspecto clave de etl es su enfoque en la reproducibilidad. Todos los scripts est√°n dise√±ados para ser modulares y parametrizables, facilitando la colaboraci√≥n entre investigadores y la actualizaci√≥n consistente de la informaci√≥n. Por ejemplo, promovemos el uso de c√≥digo que se adapte autom√°ticamente a cambios en los nombres de columnas a lo largo del tiempo, asegurando que los scripts sigan funcionando con nuevas versiones de las fuentes.\nLos resultados finales de estos procesos de etl se almacenan directamente en el repositorio data/, listos para alimentar las visualizaciones en Argendata.\n\n\n\nEl paquete argendataR (https://github.com/argendatafundar/argendataR) es una colecci√≥n de funciones en R dise√±ada para simplificar y estandarizar el flujo de trabajo con las fuentes de datos y los outputs generados en el proceso de armonizaci√≥n (ETL). Este paquete es una herramienta indispensable para investigadores y el equipo de datos, ya que agiliza tareas repetitivas y asegura la consistencia en el manejo de la informaci√≥n.\nPor ejemplo, una de sus funcionalidades centrales es la gesti√≥n de fuentes de datos, dividida en funciones para fuentes raw (originales) y clean (procesadas). Permite registrar y actualizar metadatos de las fuentes en Google Sheets, as√≠ como cargar y descargar archivos de Google Drive. Por ejemplo, con agregar_fuente_raw() y actualizar_fuente_raw(), los investigadores pueden gestionar las versiones de los datos brutos, mientras que agregar_fuente_clean() y actualizar_fuente_clean() se encargan de las versiones ya limpias y preparadas, asegurando que cumplan con los est√°ndares de Argendata (ej., nombres de columnas consistentes, formato long).\nR\n# Ejemplo de c√≥mo se registra una nueva fuente raw por primera vez \n\nlibrary(argendataR)\n## basic example code\nagregar_fuente_raw(url = \"https://www.ejemplo.com\",\n                   nombre = \"Ejemplo\",\n                   institucion = \"Ejemplo\",\n                   actualizable = TRUE,\n                   fecha_descarga = \"2021-01-01\",\n                   fecha_actualizar = \"2022-01-01\",\n                   path_raw = \"ejemplo.csv\",\n                   script = \"ejemplo.R\")\nAdem√°s, argendataR es clave en la generaci√≥n de los outputs finales. Su funci√≥n principal, write_output(), es crucial para generar los archivos .json con datos y metadatos que Argendata necesita para sus visualizaciones. Opcionalmente, tambi√©n puede exportar un .csv siguiendo los est√°ndares de Fundar (UTF-8, delimitadores espec√≠ficos), lo que garantiza la compatibilidad y facilidad de uso de nuestros datasets. Esta funci√≥n encapsula toda la complejidad del formato de salida, permitiendo a los investigadores centrarse en el procesamiento de los datos.\nR\n# Ejemplo simplificado de uso de write_output para un dataset # df_output ser√≠a el data frame ya procesado y listo \n\nwrite_output( data = df_output,\n                extension = 'csv',\n                output_name = 'A1_inb_pib',\n                subtopico = 'ACECON',\n                fuentes = 'World Development Indicators',\n                analista = 'Andr√©s Salles',\n                aclaraciones = NULL,\n                exportar = TRUE,\n                pk = c(\"anio\", \"iso3\"),\n                es_serie_tiempo = TRUE,\n                columna_indice_tiempo = \"anio\",\n                columna_geo_referencia = \"iso3\",\n                nivel_agregacion = \"pais\",\n                nullables = FALSE,\n                etiquetas_indicadores = list(\"diferencia_inb_pbi\" = \"Diferencia entre Ingreso Bruto Nacional y PBI\"),\n                unidades = \"Porcentaje respecto al PBI\",\n                classes = NULL)\nEn resumen, argendataR es la caja de herramientas para el flujo de trabajo de datos en Argendata, promoviendo la sistematizaci√≥n, la calidad y la eficiencia en la producci√≥n de conocimiento. Las salidas finales de estos procesos son las que se comparten y actualizan en el repositorio data y alimentan el sitio argendata.fund.ar para la descarga de datos.\n\n\n\nEl repositorio data (https://github.com/argendatafundar/data) es el punto central de almacenamiento y distribuci√≥n de los datasets finales que se publican en Argendata. Estos conjuntos de datos son el resultado del proceso semi-automatizado de reproducibilidad (ETL), lo que asegura su consistencia y calidad. B√°sicamente, data es la vidriera donde se consolidan y comparten los datos producidos por los investigadores, listos para su uso y descarga.\n\n\nPara mantener la claridad y facilitar el acceso, los datos en este repositorio est√°n meticulosamente organizados:\n\nCodificaci√≥n por T√≥pico: Cada t√≥pico de Argendata tiene un c√≥digo √∫nico de seis letras en may√∫sculas (ej., CAMCLI para ‚ÄúCambio Clim√°tico‚Äù, PRECIO para ‚ÄúInflaci√≥n‚Äù). Estos c√≥digos sirven como identificadores √∫nicos y estructuran el sistema de archivos del repositorio, agrupando los datasets por su tema principal.\nArchivos por Dataset: Dentro de la carpeta de cada t√≥pico, se encuentran dos tipos de archivos para cada dataset individual:\n\nEl dataset original limpio en formato .csv. Estos son los datos ya procesados y listos para ser utilizados.\nLos metadatos en formato .json, que acompa√±an a cada dataset con el mismo nombre. Estos archivos .json son cruciales porque proveen informaci√≥n detallada sobre el dataset, incluyendo sus fuentes (fuentes), tipo de extensi√≥n (extension), analistas responsables (analista), los tipos de datos por variable y otras especificaciones necesarias para su correcta interpretaci√≥n y uso.\n\nPor ejemplo, el dataset de inflaci√≥n PRECIO/12_tasa_de_inflacion_mensual_argentina_ene1989_dic1993 viene acompa√±ado de su .csv con las columnas fecha e inflacion_mensual, y un .json con sus metadatos (subtopico, output_name, extension, analista, fuentes). Este esquema garantiza que los usuarios no solo obtengan los datos, sino tambi√©n el contexto y la informaci√≥n necesaria para entenderlos y utilizarlos correctamente.\n\n\n\n\nLa organizaci√≥n del repositorio sigue un esquema l√≥gico que refleja la clasificaci√≥n por t√≥pico:\n‚îú‚îÄ‚îÄ TOPICO\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ CAMCLI\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.csv\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.json\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ COMEXT\n    ‚îú‚îÄ‚îÄ cambio_destinos_exportacion.csv\n    ‚îú‚îÄ‚îÄ cambio_destinos_exportacion.json\n    ...\nEsta estructura jer√°rquica facilita la navegaci√≥n y la identificaci√≥n de los conjuntos de datos, permitiendo a los usuarios encontrar r√°pidamente la informaci√≥n de su inter√©s.\nLos datos almacenados en este repositorio son los que se disponibilizan directamente para la descarga de los usuarios desde el sitio web de Argendata. Adem√°s, son la base para el procesamiento posterior realizado por el repositorio transformers, que los adapta para la visualizaci√≥n interactiva en el Frontend de argendata.fund.ar.\n\n\n\n\nEl repositorio data-transformers (https://github.com/argendatafundar/data-transformers) es una biblioteca de Python dise√±ada para ser un componente clave en el an√°lisis y procesamiento manual-asistido de datos estructurados dentro de Argendata. Su prop√≥sito principal es facilitar la escritura, ejecuci√≥n, reproducibilidad y el versionado del c√≥digo fuente que manipula estos datos.\nEsta herramienta se enfoca en el formato y la preparaci√≥n de los datos seg√∫n las necesidades espec√≠ficas del c√≥digo utilizado por el Frontend para las visualizaciones. No se trata de la generaci√≥n inicial de datos (eso ocurre en etl), sino de las transformaciones finales que optimizan los datasets para su presentaci√≥n interactiva. Esto puede incluir tareas como:\n\nFiltrado de valores: Seleccionar subconjuntos de datos relevantes para una visualizaci√≥n particular.\nSelecci√≥n y renombramiento de variables: Ajustar los nombres de las columnas o seleccionar solo aquellas que son necesarias para el gr√°fico final.\nAgregaciones o pivotes espec√≠ficos: Reestructurar los datos para que se adapten a los requisitos del componente de visualizaci√≥n.\n\nAl centralizar estas transformaciones en data-transformers, se garantiza que el paso final de preparaci√≥n de datos sea consistente y que cualquier cambio necesario en el formato de visualizaci√≥n se aplique de manera eficiente y reproducible. Esto es vital para mantener la agilidad en el desarrollo de nuevas visualizaciones y la calidad de la informaci√≥n mostrada en el sitio.\n\n\n\nEl repositorio transformers (https://github.com/argendatafundar/transformers) contiene los scripts en Python que act√∫an como ‚Äúrecetas‚Äù ejecutables y autocontenidas. B√°sicamente, son programas dise√±ados para aplicar una serie de instrucciones de mutaci√≥n a un dataset, transform√°ndolo espec√≠ficamente para su visualizaci√≥n en el sitio web de Argendata.\nEstos scripts van un paso m√°s all√° de la armonizaci√≥n de datos que se realiza en el proceso etl. Su objetivo es una normalizaci√≥n m√°s estricta, donde se deja √∫nicamente la informaci√≥n imprescindible para que el gr√°fico se pueda visualizar. As√≠, se generan archivos mucho m√°s livianos y optimizados para el Frontend.\nCada transformer se explica por s√≠ mismo en su funcionalidad: describe lo que hace, lo que facilita mucho su comprensi√≥n y garantiza su f√°cil ejecuci√≥n en un proceso automatizado. La relaci√≥n con data-transformers es directa: data-transformers es la librer√≠a que provee las funciones base que estos scripts utilizan para construir sus ‚Äúrecetas‚Äù.\nEl trabajo en este repositorio se organiza por t√≥picos, reflejando la estructura de Argendata:\n‚îú‚îÄ‚îÄ TOPICO\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ AGROPE\n‚îÇ   ‚îú‚îÄ‚îÄ mappings.json\n‚îÇ   ‚îú‚îÄ‚îÄ TOPICO_gXX_transformer.py\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ TRANEN\n    ‚îú‚îÄ‚îÄ mappings.json\n    ‚îú‚îÄ‚îÄ TRANEN_g01_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g02_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g03_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g04_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g05_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g06_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g07_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g08_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g09_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g10_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g11_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g12_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g13_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g14_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g15_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g16_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g17_transformer.py\n    ‚îú‚îÄ‚îÄ TRANEN_g18_transformer.py\n    ‚îî‚îÄ‚îÄ TRANEN_g19_transformer.py\n\nCada dataset publicado en data (resultado del proceso ETL) tiene una correspondencia con uno o m√°s transformers aqu√≠. Un dataset de data (por ejemplo, matriz_prim_mundo_historic_larga.csv en data) es el ‚Äúinsumo‚Äù para un transformer (como TRANEN_g01_transformer.py en transformers. Esta equivalencia est√° documentada en un archivo mappings.json dentro de cada subdirectorio de t√≥pico en este repositorio.\nUn ejemplo de c√≥mo opera un transformer es el TRANEN_g01_transformer.py, que toma un DataFrame y aplica una serie de transformaciones usando funciones de data-transformers (como query, rename_cols y sort_values) para preparar los datos para la visualizaci√≥n final. Esto asegura que los datos sean presentados de forma √≥ptima y coherente en la web.\n\nPython\n# Extracto del c√≥digo de TRANEN_g01_transformer.py \n\n\nfrom pandas import DataFrame\nfrom data_transformers import chain, transformer\n\n\n#  DEFINITIONS_START\n@transformer.convert\ndef query(df: DataFrame, condition: str):\n    df = df.query(condition)    \n    return df\n\n@transformer.convert\ndef rename_cols(df: DataFrame, map):\n    df = df.rename(columns=map)\n    return df\n\n@transformer.convert\ndef sort_values(df: DataFrame, how: str, by: list):\n    if how not in ['ascending', 'descending']:\n        raise ValueError('how must be either \"ascending\" or \"descending\"')\n    return df.sort_values(by=by, ascending=how == 'ascending')\n#  DEFINITIONS_END\n\n\n#  PIPELINE_START\npipeline = chain(\nquery(condition='tipo_energia != \"Total\"'),\n    rename_cols(map={'tipo_energia': 'indicador', 'valor_en_twh': 'valor'}),\n    sort_values(how='ascending', by=['anio', 'indicador'])\n)\n#  PIPELINE_END\n\n\n#  start()\n#  RangeIndex: 836 entries, 0 to 835\n#  Data columns (total 4 columns):\n#   #   Column        Non-Null Count  Dtype  \n#  ---  ------        --------------  -----  \n#   0   anio          836 non-null    int64  \n#   1   tipo_energia  836 non-null    object \n#   2   valor_en_twh  836 non-null    float64\n#   3   porcentaje    836 non-null    float64\n#  \n#  |    |   anio | tipo_energia     |   valor_en_twh |   porcentaje |\n#  |---:|-------:|:-----------------|---------------:|-------------:|\n#  |  0 |   1800 | Otras renovables |              0 |            0 |\n#  \n#  ------------------------------\n#  \n#  query(condition='tipo_energia != \"Total\"')\n#  Index: 760 entries, 0 to 759\n#  Data columns (total 4 columns):\n#   #   Column        Non-Null Count  Dtype  \n#  ---  ------        --------------  -----  \n#   0   anio          760 non-null    int64  \n#   1   tipo_energia  760 non-null    object \n#   2   valor_en_twh  760 non-null    float64\n#   3   porcentaje    760 non-null    float64\n#  \n#  |    |   anio | tipo_energia     |   valor_en_twh |   porcentaje |\n#  |---:|-------:|:-----------------|---------------:|-------------:|\n#  |  0 |   1800 | Otras renovables |              0 |            0 |\n#  \n#  ------------------------------\n#  \n#  rename_cols(map={'tipo_energia': 'indicador', 'valor_en_twh': 'valor'})\n#  Index: 760 entries, 0 to 759\n#  Data columns (total 4 columns):\n#   #   Column      Non-Null Count  Dtype  \n#  ---  ------      --------------  -----  \n#   0   anio        760 non-null    int64  \n#   1   indicador   760 non-null    object \n#   2   valor       760 non-null    float64\n#   3   porcentaje  760 non-null    float64\n#  \n#  |    |   anio | indicador        |   valor |   porcentaje |\n#  |---:|-------:|:-----------------|--------:|-------------:|\n#  |  0 |   1800 | Otras renovables |       0 |            0 |\n#  \n#  ------------------------------\n#  \n#  sort_values(how='ascending', by=['anio', 'indicador'])\n#  Index: 760 entries, 76 to 227\n#  Data columns (total 4 columns):\n#   #   Column      Non-Null Count  Dtype  \n#  ---  ------      --------------  -----  \n#   0   anio        760 non-null    int64  \n#   1   indicador   760 non-null    object \n#   2   valor       760 non-null    float64\n#   3   porcentaje  760 non-null    float64\n#  \n#  |    |   anio | indicador       |   valor |   porcentaje |\n#  |---:|-------:|:----------------|--------:|-------------:|\n#  | 76 |   1800 | Biocombustibles |       0 |            0 |\n#  \n#  ------------------------------\n#  \n\nEste proceso en dos etapas (primero ETL, luego transformaci√≥n para Frontend) asegura la calidad del dato base y al mismo tiempo la flexibilidad y eficiencia en la presentaci√≥n visual de la informaci√≥n.\n\n‚Äî‚Äî‚Äì\n\n\n\nHasta ac√° un resumen de lo que fue el primer a√±o, centrado en el camino de los datos. En pr√≥ximas publicaciones, espero con blogueros inivtados, avanzaremos con m√°s detalle sobre algunos de los procesos, con especial atenci√≥n al trabajo dedicado a la visualizaci√≥n.\n\nHasta la pr√≥xima y feliz cumple a nosotros =)"
  },
  {
    "objectID": "posts/Geofacetear.html",
    "href": "posts/Geofacetear.html",
    "title": "GeofaceteAR",
    "section": "",
    "text": "En este posteo quiero mostrar brevemente el proyecto geofacet_AR, presentado en la primera edici√≥n de LatinR, tal como adelant√© en el primer post. El mismo es una \\(extensi√≥n^2\\) (del proyecto de Ryan Hafen y, en otro nivel, de las grillas o facet de ggplot2).\nMe permito traducir el siguiente concepto de la nota introductoria del propio Hafen:\nA continuaci√≥n presetnar√© con un ejemplo sencillo cuales (creo) son las mayores ventajas de geofacet. A saber:"
  },
  {
    "objectID": "posts/Geofacetear.html#footnotes",
    "href": "posts/Geofacetear.html#footnotes",
    "title": "GeofaceteAR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚Äúes el n√∫mero hipot√©tico de partidos de igual tama√±o que tendr√≠an el mismo efecto total en la fraccionalizaci√≥n del sistema que el n√∫mero ‚Äòreal‚Äô [actual] de tama√±o desigual‚Äù Laakso & Taagepera (1979)‚Ü©Ô∏é"
  }
]